<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>窗外临街</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://peihao.space/"/>
  <updated>2017-03-10T07:14:08.844Z</updated>
  <id>http://peihao.space/</id>
  
  <author>
    <name>培豪</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>周报(1)</title>
    <link href="http://peihao.space/2017/03/10/week-report310/"/>
    <id>http://peihao.space/2017/03/10/week-report310/</id>
    <published>2017-03-10T04:00:13.000Z</published>
    <updated>2017-03-10T07:14:08.844Z</updated>
    
    <content type="html">&lt;h1 id=&quot;周报3-10&quot;&gt;周报3.10&lt;/h1&gt;&lt;p&gt;这次周报，主要回顾从周一下午在公司开完会回来后的工作情况以及状态。&lt;/p&gt;
&lt;h2 id=&quot;状态&quot;&gt;状态&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;时间：由于每天坚持健身锻炼，以及在临睡前习惯的观看某栏目的逻辑思维导论，拖拖拉拉的晚上会睡的比较迟。早上也是醒的蛮晚。这些习惯会在月底前的一周里改正。&lt;/li&gt;
&lt;/ul&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;地点：现在每天宿舍里空调打孔，噪音比较大。除了早上在寝室学习之外，之前没在实验室的会议室学习前，都会到教三的三楼自习。周二开始在保密会议室内学习，晚上九点半离开。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;效率：工作效率方面，白天的学习效率很低，经常会被外界的环境影响；在封闭的实验室内效率提高不少，特别是晚上六点之后。deadline是第一生产力，那个时候开始有时间危机，想着不能又白白浪费一天时间，逐渐不去理会外界干扰。同时在会议室的办公桌上可以使用外接的大屏显示器辅助工作，感觉很棒。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;工作情况&quot;&gt;工作情况&lt;/h2&gt;&lt;p&gt;目前是三月份，导师要我们9月份之前确定论文的题目，留给我们几乎半年的时间。所以这一阶段的前几十天我主要目标是在导师的指引下，在自己感兴趣的、或者认为有前景的领域粗粗的学一学，领略一番，期望遇到自己有深入进去欲望的方向、点。&lt;/p&gt;
&lt;p&gt;最近一直在看南大周教授的《机器学习》和李航老师的《统计学习方法》。第一遍读不求甚解，没有公示的推倒，只是在顺着作者的思路学习，目的就是了解这一领域的基础知识、原理与技术。&lt;/p&gt;
&lt;p&gt;两本书都对数学有一定的要求。上学期的《概率论与随机过程》课程中的部分章节在这里派上了用场，但是还有一些需要用到本科时的内容，包括《概率论与数理统计》、《线性代数》甚至《高级数学》。由于我没有参加统考，没有对这几门课程有系统的复习，几年过去，说实话遗忘了很多。类似协方差$Cov(\vec{x},\vec{y})$，特征值$\lambda$，矩阵的秩、迹、线性空间、欧氏空间、超平面等等很多定义、用法都要用到。这些有的是我学过还记得，有的学过忘掉了，还有部分没有接触过的，往往一个书上式子的理解都需要很长时间。不过嘛，这是我必经的路。前天听了同学的介绍，准备在网络上听一些课程，期望有所好转。&lt;/p&gt;
&lt;p&gt;这期间开始使用latex。&lt;/p&gt;
&lt;p&gt;周四，也就是昨天参加了在《中欧在5G及物联网领域的合作》会议。主要内容就是5G的最新进展以及物联网领域的突飞猛进。会议基本以英文演说为主，很多由于英文水平有限没有很好的理解，不过几个突出表达的新技术名词记录下来，回来查看资料还是大开眼见。几个关键词：光通信LIFI、AIOTI物联网组织、ICT合作。&lt;/p&gt;
&lt;p&gt;昨天拿到了上届学长学姐的开题报告，到写周报前，只是把题目、时间安排简单的浏览了一下。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;周报3-10&quot;&gt;周报3.10&lt;/h1&gt;&lt;p&gt;这次周报，主要回顾从周一下午在公司开完会回来后的工作情况以及状态。&lt;/p&gt;
&lt;h2 id=&quot;状态&quot;&gt;状态&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;时间：由于每天坚持健身锻炼，以及在临睡前习惯的观看某栏目的逻辑思维导论，拖拖拉拉的晚上会睡的比较迟。早上也是醒的蛮晚。这些习惯会在月底前的一周里改正。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="周报" scheme="http://peihao.space/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
      <category term="周报" scheme="http://peihao.space/tags/%E5%91%A8%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>降维</title>
    <link href="http://peihao.space/2017/03/09/ml-dimension-reduction/"/>
    <id>http://peihao.space/2017/03/09/ml-dimension-reduction/</id>
    <published>2017-03-09T02:36:28.000Z</published>
    <updated>2017-03-11T04:28:13.049Z</updated>
    
    <content type="html">&lt;h1 id=&quot;k近邻学习&quot;&gt;k近邻学习&lt;/h1&gt;&lt;p&gt;k近邻（k-Nearest Neighboor，KNN）是一种常用的监督学习方法：给定测试样本，基于某种距离距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个训练样本的信息对本样本进行预测。分类学习中通常使用投票法，少数服从多数；回归学习中则使用平均法。通常为了体现出靠近的距离指标，在分类学习与回归学习中使用加权法，越靠近样本的权值越大。&lt;/p&gt;
&lt;p&gt;给定测试样本x，若其最近邻样本为z，则最近邻分类器出错的概率就是x与z类别标记不同的概率：$P(err)=1-\sum\limits_{c \in y}P(c \mid x)P(c \mid z)$&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;假设样本独立同分布，且对任意x和任意小正数$\delta$，在x附近$\delta$距离范围内找到一个训练样本z；令$c^T=arg \max\limits_{c \in y}P(c \mid x)$表示贝叶斯最优分类器的结果，有：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(err)=1-\sum\limits_{c \in y}P(c \mid x)P(c \mid z)&lt;br&gt;\approx1-\sum\limits_{c \in y}P^2(c \mid x)&lt;br&gt;\leq 1- P^2(c^T \mid x)&lt;br&gt;=(1+P(c^T \mid x))(1-P(c^T \mid x))&lt;br&gt;\leq 2 \times (1-p(c^T \mid x))&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;可以看出，在这种理想条件下KNN分类器的泛化错误率不超过贝叶斯错误率的两倍。&lt;/p&gt;
&lt;h1 id=&quot;降维&quot;&gt;降维&lt;/h1&gt;&lt;p&gt;上面的讨论的基础是在足够小的距离内都有相邻的样本作为参考，考虑到做个属性（也就是多个维度）的情况，需要的合适的样本数量回事天文数字。高维度空间会给距离计算带来很大麻烦。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro9-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;一般采用的方式是通过某种数学变换将原始高维属性空间转变为一个低维子空间。子空间中样本密度大幅提升。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro9-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;MDS&quot;&gt;MDS&lt;/h2&gt;&lt;p&gt;MDS即多维缩放(Multiple Dimensional Scaling)，是要就将原始空间中样本间的距离在低维空间中得以保持。&lt;/p&gt;
&lt;p&gt;假定m个样本在原始空间的距离矩阵为$D \in R^{m \times m}$，在第i行j列的元素$dist_{ij}$为样本$x_i$到$s_j$的距离。目标是获得样本在$d^c$维空间的表示$Z \in R^{d^c \times m},d^c \leq d$，且任意两个样本在$d^c$维空间的欧式距离等于原始空间$\mid\mid z_i-z_j \mid\mid = dist_{ij}$&lt;/p&gt;
&lt;p&gt;令$B=Z^TZ \in R^{m \times m}$，其中B为降维后样本的内积矩阵，$b_{ij}=b_{ji}=z_i^Tz_j$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;dist_{i.}^2=\frac{1}{m}\sum\limits_{j=1}^m dist_{ij}^2&lt;br&gt;dist_{.j}^2=\frac{1}{m}\sum\limits_{i=1}^m dist_{ij}^2&lt;br&gt;dist_{..}^2=\frac{1}{m^2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m dist_{ij}^2&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;由以上的式子推出：&lt;/p&gt;
&lt;p&gt;$b_ij=-\frac{1}{2}(dist_{ij}^2-dist_{i.}^2-dist_{.j}^2+dist_{..}^2)$&lt;/p&gt;
&lt;p&gt;依次求出矩阵B中所有的元素，然后对矩阵做特征值分解：$B=V\Lambda V^T$其中V为特征向量矩阵，$\Lambda$为特征值构成的对角矩阵。假定其中有$d^c$个非零特征值，构成对角矩阵$\Lambda_x=diag(\lambda_1,\lambda_2,…,\lambda_{dx})$，令$Vx$表示相应的特征向量矩阵，则：&lt;/p&gt;
&lt;p&gt;$Z=\Lambda_c^{1/2}V_c^T \in R^(d^c \times m)$&lt;/p&gt;
&lt;h1 id=&quot;主成分分析&quot;&gt;主成分分析&lt;/h1&gt;&lt;p&gt;通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。&lt;/p&gt;
&lt;p&gt;主成份（Principal Component Analysis）分析是降维（Dimension Reduction）的重要手段。每一个主成分都是数据在某一个方向上的投影，在不同的方向上这些数据方差Variance的大小由其特征值（eigenvalue）决定。一般我们会选取最大的几个特征值所在的特征向量（eigenvector），这些方向上的信息丰富，一般认为包含了更多我们所感兴趣的信息。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在很多情形，变量之间是有一定的相关关系的，当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。主成分分析是对于原先提出的所有变量，将重复的变量（关系紧密的变量）删去多余，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;设法将原来变量重新组合成一组新的互相无关的几个综合变量，同时根据实际需要从中可以取出几个较少的综合变量尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，是数学上用来降维的一种方法。&lt;/p&gt;
&lt;p&gt;先定义一些变量：&lt;/p&gt;
&lt;p&gt;样本$(\vec{x_1},\vec{x_2},\vec{x_3},…,\vec{x_m})$&lt;/p&gt;
&lt;p&gt;投影变换得到的 &lt;strong&gt;新坐标系&lt;/strong&gt;${ \vec{\omega_1},\vec{\omega_2},…,\vec{\omega_d}, }$&lt;/p&gt;
&lt;p&gt;样本点在低维坐标系中的投影是$z_i=(z_{i1};z_{i2};z_{i3};…;z_{id_‘};)$&lt;/p&gt;
&lt;p&gt;其中$z_{ij}=\vec{\omega}_j^T\vec{x_i}$是x在低维坐标系下的第j维坐标&lt;/p&gt;
&lt;p&gt;投影点方差是$\sum_i W^Tx_ix_i^TW$ 优化目标是最大化这个方差，使得在投影之后尽可能的分散&lt;/p&gt;
&lt;p&gt;PCA一般有以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;数据减去均值：样本中心化$\sum_i \vec{x_i}=\vec{0}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算协方差矩阵：$C=\frac{XX^T}{N}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算协方差矩阵的特征矢量和特征值：对协方差矩阵$\frac{XX^T}{N}$特征分解$CU=U\Lambda$，其中C为方差矩阵，U为计算的特征矢量，$\Lambda$是对角线矩阵$\Lambda=diag(\lambda_1,\lambda_2,…,\lambda_d)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;选择成分组成模式矢量&lt;/strong&gt;对应最大特征值的特征矢量就是数据的主成分：取最大的d个特征值所对应的特征向量$u_1,u_2,…,u_d$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一般地，从协方差矩阵找到特征矢量以后，下一步就是按照特征值由大到小进行排列，这将给出成分的重要性级别。现在，如果你喜欢，可以忽略那些重要性很小的成分，当然这会丢失一些信息，但是如果对应的特征值很小，你不会丢失很多信息。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;获得投影的新数据$y_i=U_k^T x_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;核化线性降维&quot;&gt;核化线性降维&lt;/h1&gt;&lt;p&gt;线性降维假设从高维空间到低维度空间的函数映射是线性的，然而在不少的现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。&lt;/p&gt;
&lt;p&gt;非线性降维的一种常用方法是基于核技巧对线性降维方法进行”核化”。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://zhanxw.com/blog/2011/02/kernel-pca-%E5%8E%9F%E7%90%86%E5%92%8C%E6%BC%94%E7%A4%BA/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;在KPCA中&lt;/a&gt;，除了在PCA中的一些先决条件外，我们认为原有的数据有更高的维数，我们可以在更高的维度空间中做PCA分析（即在更高维里，把原始数据向不同方向进行投影）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们拿到样本点，需要将它映射到高维空间中，然后使用PCA算法进行降维。&lt;/p&gt;
&lt;p&gt;假定我们将在高维特征空间中把数据投影到由W确定的超平面上，即PCA欲求解$(\sum\limits_{i=1}^m)\vec(W)=\lambda \vec{W}$  （拉格朗日乘子法）&lt;/p&gt;
&lt;p&gt;其中$z_i$是样本点在高维特征空间的像，有$\vec{W}=\sum\limits_{i=1}^m z_i \alpha _i$&lt;/p&gt;
&lt;p&gt;$\alpha_i=\frac{1}{\lambda}z_i^T \vec{W}$&lt;/p&gt;
&lt;p&gt;z是由原始属性空间中的样本点x通过映射$\phi$产生。但是在映射到高维空间这一步，一般情况下，我们并不知道映射函数$phi$的具体形，不知道要映射到哪重维度上，于是引入核函数$\kappa(x_i,x_j)=\phi(x_i)^T\phi(x_j)$&lt;/p&gt;
&lt;p&gt;化简式子：$KA=\lambda A$，K为$\kappa$对应核矩阵，$A=(\alpha_1;\alpha_2;…;\alpha_m)$&lt;/p&gt;
&lt;p&gt;上式是特征值分解问题，取K对应的最大的d个特征值对应的特征向量。&lt;/p&gt;
&lt;p&gt;对于新样本x，投影的第j维坐标为$z_j=\omega_j^T \phi(x)=\sum\limits_{i=1}^m \alpha_i^j \kappa(x_i,x)$&lt;/p&gt;
&lt;h1 id=&quot;流形学习&quot;&gt;流形学习&lt;/h1&gt;&lt;p&gt;流形是一类借鉴了拓扑流形概念的降维方法，在局部有欧式空间的性质，能用欧式距离来进行距离计算。&lt;/p&gt;
&lt;p&gt;若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在局部上仍具有欧式空间的性质。因此可以容易的在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。&lt;/p&gt;
&lt;h2 id=&quot;等度量映射&quot;&gt;等度量映射&lt;/h2&gt;&lt;p&gt;等度量映射（Isometric Mapping）认为低维流形嵌入到高维空间后，直接在高维空间中计算直线距离具有误导性，因为高维空间中的直线距离往往在低维嵌入流形上是不可达的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro9-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;对每个点基于欧式距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻之间不存在连接，于是计算两点间测地线距离的问题，转变为了计算近邻点之间最短路径问题。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确定$x_i$的k近邻，并将$x_i$与k近邻之间的距离设置为欧氏距离，与其他点的距离设置为无穷大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;调用最短路径算法计算任意来那个样本之间的距离$dist(x_i,dist_j)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将$dist(x_i,dist_j)$作为MDS算法的输入求解&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;局部线性嵌入&quot;&gt;局部线性嵌入&lt;/h2&gt;&lt;p&gt;局部线性嵌入（Locally Linear Embedding，LEE）试图保持邻域内样本之间的线性关系。假定样本点$x_i$的坐标能通过他的邻域样本$x_j,x_k,x_l$的坐标通过线性组合组成，$x_i=w_{ij}x_j+w_{ik}x_k+w_{il}x_l$&lt;/p&gt;
&lt;p&gt;LLE希望关系在低维空间中得以保持。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro9-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;LLE算法的主要原理就是先在高维空间，计算出相应的重构系数序列$W={w_1,w_2,…,_m}$，随后在低维空间中通过相同的重构系数获取投影点。&lt;/p&gt;
&lt;p&gt;令$Z=(z_1,z_2,…,z_m) \in R^{d^. \times m}$&lt;/p&gt;
&lt;p&gt;$M = (I-W)^T(I-W)$则寻求最小化：$\min\limits_z tr(ZMZ^T)$，约束条件$ZZ^T=I$&lt;/p&gt;
&lt;p&gt;上式通过特征值分解，M最小的$d^.$个特征值对应的特征向量组成的矩阵就是$Z^T$，即在低维的投影&lt;/p&gt;
&lt;h1 id=&quot;度量学习&quot;&gt;度量学习&lt;/h1&gt;&lt;p&gt;对两个d维样本$x_i  x_j$，他们之间的平方欧式距离可写为：$dist^2(x_i,x_j)=dist_{ij,1}^2+dist_{ij,2}^2+…+dist_{ij,d}^2$&lt;/p&gt;
&lt;p&gt;其中$dist_{ij,k}$表示$x_i,x_j$在第k维上的距离，若嘉定不同属性的重要程度不一样，可以引入属性权重$\omega$&lt;/p&gt;
&lt;p&gt;$dist^2(w_i,x_j)=\omega_1\cdot dist_{ij,1}^2+\omega_2\cdot dist_{ij,2}^2+…+\omega_d\cdot dist_{ij,d}^2=(x_i-x_j)^T W(x_i-x_j)$&lt;/p&gt;
&lt;p&gt;其中$\omega_i \geq 0,W=diag(\omega)$是一个对角矩阵&lt;/p&gt;
&lt;p&gt;W是对角矩阵，坐标轴正交，属性之间无关；然而现实中并不是这样将W图换位一个普通的半正定对称矩阵M，得到马氏距离。&lt;/p&gt;
&lt;p&gt;其中M称作“度量矩阵”，而度量学习是对M进行学习。M必须是正定对称的，即必有正交基P使得M能写成$M=PP^T$&lt;/p&gt;
&lt;p&gt;对M的学习，我们需要把M直接嵌入到需要提高效率的评价指标中，通过优化指标求得M&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;k近邻学习&quot;&gt;k近邻学习&lt;/h1&gt;&lt;p&gt;k近邻（k-Nearest Neighboor，KNN）是一种常用的监督学习方法：给定测试样本，基于某种距离距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个训练样本的信息对本样本进行预测。分类学习中通常使用投票法，少数服从多数；回归学习中则使用平均法。通常为了体现出靠近的距离指标，在分类学习与回归学习中使用加权法，越靠近样本的权值越大。&lt;/p&gt;
&lt;p&gt;给定测试样本x，若其最近邻样本为z，则最近邻分类器出错的概率就是x与z类别标记不同的概率：$P(err)=1-\sum\limits_{c \in y}P(c \mid x)P(c \mid z)$&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>聚类</title>
    <link href="http://peihao.space/2017/03/08/ml-clustering/"/>
    <id>http://peihao.space/2017/03/08/ml-clustering/</id>
    <published>2017-03-08T05:18:42.000Z</published>
    <updated>2017-03-08T02:33:48.630Z</updated>
    
    <content type="html">&lt;p&gt;机器学习可以根据样本是否有标签分类为有监督学习与无监督学习，前面介绍的基本都是有监督学习。无监督学习中，目标通过对无标记训练样本的学习来揭示数据的内在性质及规律。&lt;/p&gt;
&lt;p&gt;聚类试图将数据集中的样本划分为若干个通常不相交的子集，每个子集称为一个簇（cluster）。通过这样的划分，每个簇可能对应与一些潜在的类别。这些类别两两不相交，并起来是整个数据集，聚类的结果就是产生一个标签结果序列。 &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;性能度量&quot;&gt;性能度量&lt;/h1&gt;&lt;p&gt;对聚类来讲，我们需要通过某种性能来评估聚类效果的好坏；若明确了最终要使用的性能度量，可以直接将其作为聚类过程的优化目标。&lt;/p&gt;
&lt;p&gt;聚类性能度量大致有两类，一类是将聚类结果与某个参考模型进行比较，称为外部指标；另一类是直接考察聚类结果而不利用任何参考模型，称为内部指标。&lt;/p&gt;
&lt;h2 id=&quot;外部指标&quot;&gt;外部指标&lt;/h2&gt;&lt;p&gt;对数据集$D={x_1,x_2,…,x_m}$，假定通过聚类给出额簇划分为$C={C_1,C_2,…,C_k}$，参考模型给出的簇划分为$C^`={C_1^T,C_2^T,…,C_s^T}$。相应的，令$\lambda$与$\lambda^T$分别表示与C和$C^T$对应的簇标记向量。注意的是，参考模型给出的划分类别数量不一定等于通过聚类得到的数量。&lt;/p&gt;
&lt;p&gt;样本两两配对：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$a=\mid SS \mid ,SS={(x_i,x_j)\mid \lambda_i = \lambda_j,\lambda_i^T=\lambda_j^T,i&amp;lt;j}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$b=\mid SS \mid ,SD={(x_i,x_j)\mid \lambda_i = \lambda_j,\lambda_i^T\neq \lambda_j^T,i&amp;lt;j}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$c=\mid SS \mid ,DS={(x_i,x_j)\mid \lambda_i \neq \lambda_j,\lambda_i^T=\lambda_j^T,i&amp;lt;j}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$a=\mid SS \mid ,DD={(x_i,x_j)\mid \lambda_i \neq \lambda_j,\lambda_i^T \neq \lambda_j^T,i&amp;lt;j}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;集合SS包含了C中隶属于相同簇且在$C^`$中也隶属于相同簇的样本对，集合SD包含了在C中隶属于相同簇但在$C^T$中隶属于不同簇的样本对&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Jaccard系数：$JC=\frac{a}{a+b+c}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FM指数：$FMI=\sqrt{\frac{a}{a+b}\frac{a}{a+c}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rand指数：$RI=\frac{2(a+d)}{m(m-1)}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述性能度量的结果值均在[0,1]区间，值越大越好。&lt;/p&gt;
&lt;h2 id=&quot;内部指标&quot;&gt;内部指标&lt;/h2&gt;&lt;p&gt;考虑聚类结果的簇划分$C={C_1,C_2,…,C_k}$，定义&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$avg(C)=\frac{2}{\mid C \mid(\mid C \mid -1)}\sum_{1 \leq i &amp;lt; j \leq \mid C \mid}dist(x_i,x_j)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$diam(C)=\max_{1 \leq i &amp;lt;j \leq \mid C \mid}dist(x_i,x_j)$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$d_\min(C_i,C_j)=\min_{x_i \in C_i , x_j \in C_j} dist(x_i,x_j)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$d_cen(C_i,C_j)=dist(\mu_i,\mu_j)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的式子中，dist计算两个样本之间的距离；$\mu$代表簇的中心点$\mu=\frac{\sum_{1 \leq i \leq \mid C \mid x_i}}{\mid C \mid}$；avg(C)uiying与簇内样本间的平均距离，diam(C)对应与簇C内样本间的最远距离，$d_min(C_i,C&lt;em&gt;j)$对应与簇i和簇j最近样本间的距离；$d&lt;/em&gt;{cen}(C_i,C_j)$对应与簇i和j中心点间的距离。&lt;/p&gt;
&lt;p&gt;基于上面的指标，推出下面几个内部指标：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$DBI=\frac{1}{k}\sum\limits_{i=1}^k\max\limits_{j \neq i}(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)})$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$DI=\min\limits_{1 \leq i \leq k}{ \min\limits_{j \neq i}(\frac{d_{min}(C_i,C_j)}{\max_{1\leq l \leq k diam(C_l)}}) }$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;显然，DBI的值越小越好，DI值越大越好&lt;/p&gt;
&lt;h1 id=&quot;距离计算&quot;&gt;距离计算&lt;/h1&gt;&lt;p&gt;在讨论距离计算的时候，属性是否定义了”序”关系很重要。例如定义域为${ 1,2,3 }$能直接在属性值上计算距离，这样的属性成为有序属性；而定义域为{ 飞机、火车、轮船 }这样的离散属性则不能直接在属性值上计算距离，称为无序属性。&lt;/p&gt;
&lt;p&gt;对有序属性的距离计算，通常使用Minkowski distance：$dist_{mk}(x_i,x_j)=(\sum\limits_{u=1}^n \mid x_{iu}-x_{ju} \mid^p)^{\frac{1}{p}}$&lt;/p&gt;
&lt;p&gt;对无需属性采用VDM算式：$VDM_p(a,b)=\sum\limits_{i=1}^k\mid \frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_u,b} \mid ^p$&lt;/p&gt;
&lt;p&gt;其中$m_{u,a} m_{u,a,i}$分别表示在属性u上取值a的样本数以及在第i个样本簇中属性u上取值为a的样本数。k为样本簇数&lt;/p&gt;
&lt;p&gt;混合处理，假定有$n_c$个有序属性以及$n-n_c$个无序属性：&lt;/p&gt;
&lt;p&gt;$MinkorDM_p(x_i,x_j)=(\sum\limits_{u=1}^{n_c}\mid x_{iu}-x_{ju} \mid ^p +\sum\limits_{u=n_c+1}^n VDM_p(x_{iu},x_{ju}))^{\frac{1}{p}}$&lt;/p&gt;
&lt;p&gt;属性重要性不同时，可以加权处理&lt;/p&gt;
&lt;h1 id=&quot;原型聚类&quot;&gt;原型聚类&lt;/h1&gt;&lt;p&gt;原型聚类算法假设聚类结构能够通过一组原型刻画，通常情况下算法先对原型进行初始化，然后对原型进行迭代更新。&lt;/p&gt;
&lt;h2 id=&quot;k均值算法&quot;&gt;k均值算法&lt;/h2&gt;&lt;p&gt;给定样本集$D={x_1,x_2,…,x_m}$，k均值算法针对聚类所得簇划分${ C_1,C_2,…,C_k }$最小化平方误差。$E=\sum\limits_{i=1}^k\sum\limits_{x \in C_i}\mid\mid x-\mu_i \mid\mid_2^2$&lt;/p&gt;
&lt;p&gt;其中$\mu_i=\frac{\sum_{x \in C_i}\vec{x}}{\mid C_i \mid}$是簇$C_i$的均值向量。上式在一定程度上刻画了簇内样本围绕簇均值向量的紧密吃呢孤独，E值越小则簇内样本相似度越高。k均值采用贪心策略，通过迭代优化来近似求解上式。&lt;/p&gt;
&lt;h2 id=&quot;学习向量量化&quot;&gt;学习向量量化&lt;/h2&gt;&lt;p&gt;学习向量量化（LVQ）试图找到一组原型向量来刻画聚类结构，它假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。&lt;/p&gt;
&lt;p&gt;算法首先对原型向量进行优化，然后对原型向量进行迭代优化。在每一轮的迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行更新。&lt;/p&gt;
&lt;p&gt;在更新原型向量上，对样本$x_i$，若最近的原型向量$p_i$与$x_j$的类别标记相同，则令$p_i$向$x_j$方向靠拢；否则更新原型向量与$x_j$之间距离增大，远离$x_j$&lt;/p&gt;
&lt;h2 id=&quot;高斯混合聚类&quot;&gt;高斯混合聚类&lt;/h2&gt;&lt;p&gt;统计学习的模型有两种，一种是概率模型，一种是非概率模型。所谓概率模型，是指训练模型的形式是P(Y|X)。输入是X，输出是Y，训练后模型得到的输出不是一个具体的值，而是一系列的概率值（对应于分类问题来说，就是输入X对应于各个不同Y（类）的概率），然后我们选取概率最大的那个类作为判决对象（软分类–soft assignment）。所谓非概率模型，是指训练模型是一个决策函数Y=f(X)，输入数据X是多少就可以投影得到唯一的Y，即判决结果（硬分类–hard assignment）。&lt;/p&gt;
&lt;p&gt;高斯混合模型GMM就是指对样本的概率密度分布进行估计，而估计采用的模型（训练模型）就是几个高斯模型的加权和。每个高斯模型代表一个聚类。对样本中的数据分别在几个高斯模型上进行投影，就会分别得到在各个类上的概率，选取概率最大的类作为判决结果。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;理论上可以通过增加模型的数量，用GMM近似任何概率分布&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;混合高斯模型定义：$p(x)=\sum_{k=1}^k \pi_kp(x \mid k)$&lt;/p&gt;
&lt;p&gt;其中k为模型的个数；$\pi_k$为第k个高斯的权重；$p(x\mid k)$则为第k个高斯概率密度，其均值为$\mu_k$，方差为$\theta_k$。对此概率密度的估计就是要求出$\pi_k,\mu_k,\theta_k$。当求出p(x)的表达式后，求和式的各项的结果就分别代表样本x属于各个类的概率。&lt;/p&gt;
&lt;p&gt;在做参数估计的时候，常采用的是最大似然方法。最大似然法就是使样本点在估计的概率密度函数上的概率值最大。由于概率值一般都很小，N 很大的时候, 连乘的结果非常小，容易造成浮点数下溢。所以我们通常取log，将目标改写成：$\max\sum\limits_{i=1}^N log(\sum\limits_{k=1}^K\pi_kN(x_i \mid \mu_k,\theta_k))$&lt;/p&gt;
&lt;p&gt;一般用来做参数估计的时候，我们都是通过对待求变量进行求导来求极值，在上式中，log函数中又有求和，你想用求导的方法算的话方程组将会非常复杂，没有闭合解。可以采用的求解方法是EM算法——将求解分为两步：第一步,假设知道各个高斯模型的参数（可以初始化一个，或者基于上一步迭代结果），去估计每个高斯模型的权值；第二步,基于估计的权值，回过头再去确定高斯模型的参数。重复这两个步骤，直到波动很小，近似达到极值（注意这里是极值不是最值，EM算法会陷入局部最优）。具体表达如下：&lt;/p&gt;
&lt;p&gt;E：对滴i个样本$x_i$来说，它由第k个模型生成的概率为：$\vec{w_i}(k)=\frac{\pi_kN(x_i \mid \mu_k,\theta_k)}{\sum\limits_{j=1}^K \pi_jN(x_i \mid \mu_j,\theta_j)}$&lt;/p&gt;
&lt;p&gt;在这一步，假设高斯模型的参数是已知的，有上一步迭代而来或者由初始值决定&lt;/p&gt;
&lt;p&gt;M：得到每个点的生成概率以后，对样本$x_i$来说，他的$\vec{w}_i(k)x_i$的值是由第k个高斯模型产生的。换句话说，第k个高斯模型很产生了$\vec{w}_i(k)x_i(i=1……N)$这些数据。这样在估计第k个高斯模型参数时，我们就用$\vec{w}_i(k)x_i(i=1……N)$这些数据去做参数估计：&lt;/p&gt;
&lt;p&gt;$\mu_k=\frac{\sum\limits_{i=1}^N \vec{w}_i (k)x_i}{N}$&lt;/p&gt;
&lt;p&gt;$\theta_k=\frac{\sum\limits_{i=1}^N\vec{w}_i(k)(x_i-\mu_k)(x_i-\mu_k)^T}{N_k}$&lt;/p&gt;
&lt;p&gt;$N_k=\sum\limits_{i=1}^N\vec{w}_i(k)$&lt;/p&gt;
&lt;p&gt;重复E和M知道算法收敛&lt;/p&gt;
&lt;h1 id=&quot;密度聚类&quot;&gt;密度聚类&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$\epsilon$邻域：对象O的是与O为中心，$\epsilon$为半径的空间，参数$\epsilon &amp;gt; 0$，是用户指定每个对象的领域半径值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MinPts（领域密度阀值）：对象的$\epsilon$邻域的对象数量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;核心对象：如果对象O的$\epsilon$邻域对象数量至少包含MinPts个对象，则该对象是核心对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;直接密度可达：如果对象p在核心对象q的$\epsilon$邻域内，则p是从q直接密度可达的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;密度可达：在DBSCAN中，p是从q(核心对象)密度可达的，如果存在对象链$(p_1,p_2,…,p_n)$，使得$p_1=x_i,p_n=x_j$,且有$p_{i+1}$由$p_i$直接密度直达&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;密度相连：对$x_i$和$x_j$，若存在$x_k$使得$x_i,x_j$均由$x_k$密度可达，则两者密度相连。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro8-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;DBSCAN算法先任选数据集中的一个核心对象为种子，再由此出发确定相应的聚类簇。具体的，现根据给定的邻域参数$(\epsilon,MinPys)$找出所有的核心对象；然后以任意核心对象为出发点，找出由其密度可达的样本生成聚类簇，知道所有核心对象均被访问过为止。&lt;/p&gt;
&lt;h1 id=&quot;层次聚类&quot;&gt;层次聚类&lt;/h1&gt;&lt;p&gt;层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用自底向上的聚合策略，也可选择自顶下下的分析策略。&lt;/p&gt;
&lt;p&gt;AGNES是一种自底向上聚合策略的层次聚合算法。他先将数据集合中每个样本看作一个初始聚类簇，然后再算法运行的每一步中找出距离最近的两个聚类簇合并，该过程不断重复，直至达到预设的聚类簇个数。&lt;/p&gt;
&lt;p&gt;算法的关键是如何找到最近的两个聚类簇，然后不断的进行合并。给定聚类簇$C_i$和$C_j$，通过下面的式子计算距离：&lt;/p&gt;
&lt;p&gt;最小距离： $d_min(C_i,C_j)=\min\limits_{x \in C_,z \in C_j} dist(x,z)$&lt;/p&gt;
&lt;p&gt;最大距离： $d_max(C_i,C_j)=\max\limits_{x \in C_,z \in C_j} dist(x,z)$&lt;/p&gt;
&lt;p&gt;平均距离： $d_avg(C_i,C_j)=\frac{\sum\limits_{x \in C_i}\sum\limits_{z \in C_j}dist(x,z)}{\mid C_i \mid \mid C_j \mid}$&lt;/p&gt;
&lt;p&gt;当使用不同的距离公式作为算法的因数时，算法分别被称为单链接，全连接以及均链接。算法首先对仅含一个样本样本的初始聚类簇和相应的距离矩阵进行初始化；然后不断合并距离最近的聚类簇，并对合并得到的新聚类簇距离矩阵进行更新；不断重复，直到聚类簇数量减少到预设目标。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习可以根据样本是否有标签分类为有监督学习与无监督学习，前面介绍的基本都是有监督学习。无监督学习中，目标通过对无标记训练样本的学习来揭示数据的内在性质及规律。&lt;/p&gt;
&lt;p&gt;聚类试图将数据集中的样本划分为若干个通常不相交的子集，每个子集称为一个簇（cluster）。通过这样的划分，每个簇可能对应与一些潜在的类别。这些类别两两不相交，并起来是整个数据集，聚类的结果就是产生一个标签结果序列。
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>集成学习</title>
    <link href="http://peihao.space/2017/03/07/ml-integration/"/>
    <id>http://peihao.space/2017/03/07/ml-integration/</id>
    <published>2017-03-07T05:18:42.000Z</published>
    <updated>2017-03-06T13:43:53.926Z</updated>
    
    <content type="html">&lt;h1 id=&quot;个体集成&quot;&gt;个体集成&lt;/h1&gt;&lt;p&gt;集成学习通过构建并结合多个学习器来完成学习任务，先产生一组个体学习器，再用某种策略把他们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生。&lt;/p&gt;
&lt;p&gt;同质集成：个体全是相同类型的学习器，称为基学习器&lt;/p&gt;
&lt;p&gt;异质集成：个体可以是不同的学习器，称为组件学习器&lt;/p&gt;
&lt;p&gt;根据个体学习器的生成方式，目前的集成学习方法大致分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法Boosting；以及不存在强依赖关系，可同时生成的并行化方法Bagging和随机森林。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;欲构建泛化能力强的集成，getInstance学习器应该好而不同。&lt;/p&gt;
&lt;h1 id=&quot;Boosting&quot;&gt;Boosting&lt;/h1&gt;&lt;p&gt;先从初始训练集中训练一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注。然后基于调整后的样本分布来训练下一个基学习器，如此重复进行，直至基学习器数目达到事先制定的值T，最终将这T个基学习器进行加权结合。&lt;/p&gt;
&lt;h2 id=&quot;AdaBoost&quot;&gt;AdaBoost&lt;/h2&gt;&lt;p&gt;假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同。&lt;/p&gt;
&lt;p&gt;AdaBoost反复学习基本分类器，在每一轮,=1,2,…,M顺次的执行下面的操作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率：$e_m=p(G_m(x_i) \neq y_i)=\sum\limits_{G_m(x_i)\neq y_i}\omega_{mi}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里，$\omega_{mi}$表示第m轮中第i个实例的权值，$\sum\limits_{i=1}^N\omega_{mi}=1$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;计算基本分类器$G_m(x)$的系数$\alpha_m$表示$G_m(x)$在最终分类器中的重要性。当上面计算的$e_m \leq \frac{1}{2}$时，$\alpha \geq 0$，并且随着分类误差的减小增大，所以&lt;strong&gt;分类误差率越小的基本分类器在最终分类器中的作用越大&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;更新训练数据的权值分布为下一轮做准备,被基本分类器$G_m(x)$误分类样本的权值得到扩大，所以&lt;strong&gt;误分类样本在下一轮学习中起更大更大作用&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后通过线性组合实现M个基本分类器的加权表决，$\alpha_m$表示了基本分类器的重要性&lt;/p&gt;
&lt;h1 id=&quot;并行化方法&quot;&gt;并行化方法&lt;/h1&gt;&lt;h2 id=&quot;Bagging&quot;&gt;Bagging&lt;/h2&gt;&lt;p&gt;Bagging基于自助取样法，给定包含m个样本的数据集，先随机取出一个样本放入采样集中，在吧该样本放回到初始数据集合中，使得下次采样时候仍有可能被选中。经过m次随机采样操作，得到含有m个样本的采样集。&lt;/p&gt;
&lt;p&gt;根据上面的自助采样方法，我们得到T个含有m个训练样本（m个训练样本很有可能有重复的），然后基于每个采样集训练一个基学习器。在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测出现同样票数情况，可以随机选择一个。&lt;/p&gt;
&lt;h2 id=&quot;随机森林&quot;&gt;随机森林&lt;/h2&gt;&lt;p&gt;随机森林（Random Forest，RF）是Bagging的一个变体。RF在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。&lt;/p&gt;
&lt;p&gt;传统决策树在选择划分属性时是在当前节点的属性集合（假定有d个属性中选择一个最优属性），而在RF中对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k属性的子集，然后再从这个子集中选择一个最优属性用于划分。k决定了随机性，一般推荐$k=log_2 d$&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;个体集成&quot;&gt;个体集成&lt;/h1&gt;&lt;p&gt;集成学习通过构建并结合多个学习器来完成学习任务，先产生一组个体学习器，再用某种策略把他们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生。&lt;/p&gt;
&lt;p&gt;同质集成：个体全是相同类型的学习器，称为基学习器&lt;/p&gt;
&lt;p&gt;异质集成：个体可以是不同的学习器，称为组件学习器&lt;/p&gt;
&lt;p&gt;根据个体学习器的生成方式，目前的集成学习方法大致分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法Boosting；以及不存在强依赖关系，可同时生成的并行化方法Bagging和随机森林。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯Bayesian</title>
    <link href="http://peihao.space/2017/03/06/ml-intro7/"/>
    <id>http://peihao.space/2017/03/06/ml-intro7/</id>
    <published>2017-03-06T05:18:42.000Z</published>
    <updated>2017-03-06T04:14:39.128Z</updated>
    
    <content type="html">&lt;h1 id=&quot;贝叶斯决策论&quot;&gt;贝叶斯决策论&lt;/h1&gt;&lt;p&gt;在相关概率已知的情况下，贝叶斯决策论考虑如何基于这些概率和误判损失选择最优的类别标记。&lt;/p&gt;
&lt;p&gt;假设有N中可能的类别标记${c_1,c_2,c_3,…,c_N}, \lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为i产生的损失。推出来样本x分类为$c_i$所产生的期望损失：&lt;/p&gt;
&lt;p&gt;$R(c_i \mid x)=\sum\limits_{j=1}^N \lambda_{ij}P(c_j \mid x)$&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;显然对每个样本x，若能最小化条件风险$R(h(x)\mid x)$，即在每个样本上选择那个使条件风险$R(c\mid x)$最小的类别标记，总体风险R(h)也将最小化。&lt;/p&gt;
&lt;p&gt;$h^{*}(x)=arg_{c in y}\min R(c\mid x)$&lt;/p&gt;
&lt;p&gt;此时$h^&lt;em&gt;$称为贝叶斯最优分类器，与之对应的总体风险$R(h^&lt;/em&gt;)$称为贝叶斯风险。$1-R(h^*)$反映了分类器能达到的理论上限。&lt;/p&gt;
&lt;p&gt;若目标是最小化分类错误率，则误判损失$\lambda_{ij}$可写为：$\lambda_{ij} =0 if i==j else 1$&lt;/p&gt;
&lt;p&gt;此时的条件风险：$R(c \mid x)=1-P(c \mid x)$，于是最小化分类错误率的贝叶斯最优分类器为$h^*(x)=arg_{c \in y}\max P(c \mid x)$&lt;/p&gt;
&lt;p&gt;这里有两种策略估计后验概率$P(c \mid x)$：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;给定x，直接建模$P(c \mid x)$预测c，这种判别式模型，之前介绍的决策树、神经网络、SVM都可行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对联合分布P(x,c)建模，之后获得$P(c \mid x)$，这种生成式建模 $P(c \mid x)=\frac{P(c)P(x \mid c)}{P()x}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上式，其中$P(c)$是累先验概率，也就是预估概率；$P(x \mid c)$是样本x相对于类标记c的类条件概率，也称为似然likelihood；$P(x)$是用于归一化的证据因子。对于给定样本x，证据因子$P(x)$与类标记无关。&lt;/p&gt;
&lt;p&gt;类先验概率$P(c)$表达了样本空间中各类样本所占的比例；当训练集中包含充足的独立同分布样本时，$P(c)$可通过各类样本出现的频率估计。&lt;/p&gt;
&lt;h1 id=&quot;极大似然估计&quot;&gt;极大似然估计&lt;/h1&gt;&lt;p&gt;估计类条件概率的一种常用策略就是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。&lt;/p&gt;
&lt;p&gt;具体到上节的类条件概率上，记关于类别c的类条件概率为$P(x \mid c)$，假定$P(x \mid c)$具有确定的形式并且被参数向量$\theta_c$唯一确定，则我们的任务就是利用训练集D估计参数$\theta_c$。此时我们记$P(x \mid c)$为$P(x \mid \theta_c)$&lt;/p&gt;
&lt;h1 id=&quot;朴素贝叶斯分类器&quot;&gt;朴素贝叶斯分类器&lt;/h1&gt;&lt;p&gt;基于贝叶斯公式来估计后验概率$P(c \mid x)$的主要困难在于累条件概率$P(x \mid c)$是所有属性上的联合概率，难以从有限的训练样本直接估计得到。&lt;/p&gt;
&lt;p&gt;为了避开这个障碍，朴素贝叶斯分类采用属性条件独立性假设：对已知类别假设所有的属性相互独立。&lt;/p&gt;
&lt;p&gt;$P(c \mid x)=\frac{P(c)P(x \mid c)}{P(x)}=\frac{P(c)}{P(x)}\prod\limits_{i=1}^dP(x_i \mid c)$&lt;/p&gt;
&lt;p&gt;其中d为属性数目，$x_i$为x在第i个属性上的取值。&lt;/p&gt;
&lt;p&gt;由于对所有的类别来讲$P(x)$相同，因此贝叶斯判定准则有$h_{nb}=arg\max\limits_{c \in y}P(c)\prod\limits_{i=1}^dP(x_i \mid c)$&lt;/p&gt;
&lt;p&gt;显然朴素贝叶斯分类器的训练过程就是基于训练集D来估计类先验概率P(c)，并为ie每个属性估计条件概率$p(x_i \mid c)$&lt;/p&gt;
&lt;h2 id=&quot;小例子&quot;&gt;小例子&lt;/h2&gt;&lt;p&gt;令$D_c$表示训练集D中第c类样本组成的集合，若有充足的独立同分布样本，则可以容易的估计出类先验概率：$P(c)=\frac{\mid D_c \mid}{\mid D \mid}$&lt;/p&gt;
&lt;p&gt;对离散属性而言，令$D_{c,x_i}$表示$D_c$中在第i个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i \mid c)$可以估计为：$P(x_i \mid c)=\frac{\mid D_{c,x_i} \mid}{\mid D_c \mid}$&lt;/p&gt;
&lt;p&gt;对于连续属性，假定$p(x_i \mid c) \sim N(\mu_{c,i},\theta_{c,i}^2)$,其中$\mu_{c,i}$和$\theta_{c,i}^2$分别是第c类样本在第i个属性上取指的均值和方差。$p(x_i \mid c)=\frac{exp^({-\frac{(x_i-\mu_{c,i})^2}{2\theta_{c,i}^2}})}{\sqrt{2\pi}\theta_{c,i}}$&lt;/p&gt;
&lt;h2 id=&quot;拉普拉斯修正&quot;&gt;拉普拉斯修正&lt;/h2&gt;&lt;p&gt;如果某个属性在训练集中没有与某个类同时出现过，则直接使用上式判别将出现问题。具体表现在连乘过程中，一个变量为0，则整个式子都为0.为了避免这种情况的发生，使用拉普拉丝修正：$\hat{P(c)}=\frac{\mid D_c \mid+1}{\mid D \mid+N}$，条件概率修正为$\hat{P}(x_i \mid c)=\frac{\mid D_{c,x_i} \mid+1}{\mid D_c \mid+N_i}$&lt;/p&gt;
&lt;h1 id=&quot;半朴素贝叶斯&quot;&gt;半朴素贝叶斯&lt;/h1&gt;&lt;p&gt;放宽朴素贝叶斯分类中对属性条件独立性的要求，使得贝叶斯更加的普适性，人们在朴素贝叶斯的基础上提出了半朴素贝叶斯。&lt;/p&gt;
&lt;p&gt;半朴素贝叶斯适当的考虑一部分属性间的相互依赖关系，从而不至于彻底忽略比较强的属性依赖；但是为了避免问题陷入到复杂的联合概率计算中，一般会对属性依赖的数量有要求。&lt;/p&gt;
&lt;p&gt;假定只能最多一个依赖关系，也就是”独依赖估计”：$P(c\mid x) \propto P(c)\prod\limits_{i=1}^dP(x_i \mid c,pa_i) $&lt;/p&gt;
&lt;p&gt;其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。若每个属性的父属性都已知，则可直接使用之前的公式求解；否则我们需要确定每个属性的父属性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro7-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图是朴素贝叶斯（独、无依赖立）以及两种常见的半贝叶斯（独依赖）：SPODE（SUper Parent ODE）和TAN（Tree Augmented naive Bayes）&lt;/p&gt;
&lt;p&gt;SPODE中有一个超属性，属性如果有依赖，则都依赖到此属性上面；&lt;/p&gt;
&lt;p&gt;TAN则是在最大带权生成树算法基础上通过转换将依赖结构化简。&lt;/p&gt;
&lt;p&gt;AODE（Averaged One Dependent Estimator）尝试将每个属性作为超父来构建SPODE，然后将那些有足够训练数据支撑的SPODE继承起来作为最终结果。&lt;/p&gt;
&lt;h1 id=&quot;贝叶斯网&quot;&gt;贝叶斯网&lt;/h1&gt;&lt;p&gt;贝叶斯网B由结构G和参数$\Theta$两部分组成，即$B=\langle F,\Theta \rangle$。网络结构G是一个有向无环图，每个节点对应一个属性，若两个属性有直接的依赖关系，则由一条边连接起来；参数$\Theta$定量的描述依赖关系。例如属性$x_i$在G中的父节点集为$\pi_i$，则$\Theta$包含了每个属性的条件概率表$\theta_{x_i \mid \pi_i}=P_B(x_i \mid \pi_i)$&lt;/p&gt;
&lt;p&gt;贝叶斯网有效的表达了属性间的条件独立性。给定父节点集合，贝叶斯网假设每个属性与它的非后裔属性独立。&lt;/p&gt;
&lt;h2 id=&quot;学习&quot;&gt;学习&lt;/h2&gt;&lt;p&gt;正常情况下，我们并不知晓网络结构，需要通过学习方法建立或匹配上适当的网络结构。&lt;/p&gt;
&lt;p&gt;评分搜索：定义评分函数来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网。&lt;/p&gt;
&lt;h1 id=&quot;EM算法&quot;&gt;EM算法&lt;/h1&gt;&lt;p&gt;对于不完整的训练样本，或者说是未观测的变量，学名是隐变量：令X表示已观测变量集，Z表示隐变量集，$\Theta$表示模型参数&lt;/p&gt;
&lt;p&gt;对$\Theta$做极大似然估计，则最大化对数似然$LL(\Theta \mid X,Z)=ln P(X,Z\mid \Theta)$&lt;/p&gt;
&lt;p&gt;无法直接求解的隐变量，通过对Z计算期望，最大化已观测数据的对数“边际似然”&lt;/p&gt;
&lt;p&gt;EM算法是常用的估计参数隐变量的迭代方法：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（E）若参数$\Theta$已知，则可根据训练数据推断出最优隐变量Z的值&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（M）反之做Z值已知，可方便的对参数$\Theta$做极大似然估计&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以初始值$\Theta^0$迭代步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;基于$\Theta^t$推断隐变量Z的期望记为$Z^t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于已观测变量X和$Z^t$对参数$\Theta$做极大似然估计，记为$\Theta^{t+1}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;贝叶斯决策论&quot;&gt;贝叶斯决策论&lt;/h1&gt;&lt;p&gt;在相关概率已知的情况下，贝叶斯决策论考虑如何基于这些概率和误判损失选择最优的类别标记。&lt;/p&gt;
&lt;p&gt;假设有N中可能的类别标记${c_1,c_2,c_3,…,c_N}, \lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为i产生的损失。推出来样本x分类为$c_i$所产生的期望损失：&lt;/p&gt;
&lt;p&gt;$R(c_i \mid x)=\sum\limits_{j=1}^N \lambda_{ij}P(c_j \mid x)$&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML学习-SVM</title>
    <link href="http://peihao.space/2017/03/05/ml-intro6/"/>
    <id>http://peihao.space/2017/03/05/ml-intro6/</id>
    <published>2017-03-05T05:18:42.000Z</published>
    <updated>2017-03-03T16:38:52.462Z</updated>
    
    <content type="html">&lt;p&gt;支持向量机（Support Vector Machine）的求解通常是借助凸优化技术。&lt;/p&gt;
&lt;h1 id=&quot;间隔与支持向量&quot;&gt;间隔与支持向量&lt;/h1&gt;&lt;p&gt;给定训练样本集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)},y_i \in (-1,+1 )$  分类学习最基本的思想就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。SVM就是帮助我们寻找到众多划分超平面中最符合的。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;怎样定义符合这个指标呢，作为分类问题，训练中能够尽可能划分出不同类别的样本是基本，然后在测试集中也能表现出来很好的分类能力，对未见示例泛化能力最强。在训练中表现的就是对训练样本局部扰动容忍度最高。&lt;/p&gt;
&lt;h2 id=&quot;公式表示&quot;&gt;公式表示&lt;/h2&gt;&lt;p&gt;划分超平面可以用此式表示：&lt;/p&gt;
&lt;p&gt;$\vec{\omega}^T \vec{x} + b=0$&lt;/p&gt;
&lt;p&gt;其中$\vec{\omega}=(\omega_1;\omega_2;…;\omega_d)$为法向量，决定了超平面的方向；b为位向量，决定了超平面与原点之间的距离。&lt;/p&gt;
&lt;p&gt;样本空间中任意点到超平面x到超平面$(\vec{\omega},b)$的距离可写为：&lt;/p&gt;
&lt;p&gt;$r=\frac{\mid\vec{\omega}^T \vec{x}+b\mid}{\mid\mid \vec{\omega}\mid\mid}$&lt;/p&gt;
&lt;p&gt;设置函数：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$\omega^T x_i + b \geq +1 , y_i= +1$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\omega^T x_i + b \leq -1 , y_i= -1$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;简化两个式子：&lt;/p&gt;
&lt;p&gt;$y_i(\omega^T x_i +b) \geq 1 , i=1,2,…,m$&lt;br&gt;这个是之后式子的约束条件&lt;/p&gt;
&lt;p&gt;距离超平面最近的几个训练样本点使得上面的不等式等号成立，它们被称为支持向量(support vector)，两个异类支持向量到超平面的距离之和称为间隔(margin)&lt;/p&gt;
&lt;p&gt;$\gamma = \frac{2}{\omega}$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro6-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;为了使得间隔最大，也就是求$\max\limits_{w,b} \frac{2}{\mid\mid \omega \mid\mid}$&lt;/p&gt;
&lt;p&gt;转换为$\min\limits_{w,b}\frac{\mid\mid \omega \mid\mid^2}{2}$&lt;/p&gt;
&lt;p&gt;这就是支持向量机&lt;/p&gt;
&lt;h1 id=&quot;对偶问题&quot;&gt;对偶问题&lt;/h1&gt;&lt;p&gt;模型$f(x)=\vec{\omega}^T \vec{x}+b$，参数$\omega b$是模型参数。而$\min\limits_{w,b}\frac{\mid\mid \omega \mid\mid^2}{2}$是一个凸二次规划，除了使用现成的优化计算包外，我们可以使用数学上的对偶关系更高效的求出结果。&lt;/p&gt;
&lt;p&gt;对这个凸二次规划式子添加拉格朗日乘子$\alpha \geq 0$，则问题的拉格朗日函数可写为：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$L(\vec{\omega},b,\vec{\alpha})=\frac{1}{2}\mid\mid \omega \mid\mid^2+\sum\limits_{i=1}^m\alpha_i(1-y_i(\omega^Tx_i+b))$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;令L对$\omega b$的偏导为零可得：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$\omega=\sum\limits_{i=1}^m \alpha_iy_ix_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$0=\sum\limits_{i=1}^m\alpha_iy_i$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;式1代入拉格朗日式子，式2作为约束函数，得到上一小节式子的对偶问题：&lt;/p&gt;
&lt;p&gt;$\max\limits_{\alpha}         \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_jx_i^T x_j$&lt;/p&gt;
&lt;p&gt;约束条件：$\sum\limits_{i=1}^m\alpha_iy_i=0$&lt;/p&gt;
&lt;p&gt;上式是南教授《机器学习》书中记录的式子，这里认为下面式子可能更好理解&lt;/p&gt;
&lt;p&gt;$\max\limits_{\alpha}         \sum_{i=1}^m\alpha_i+\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_jx_i^T x_j$&lt;/p&gt;
&lt;p&gt;从上式解出$\vec{\alpha}$之后，求出$\vec{\omega}$和b即可得到模型：&lt;/p&gt;
&lt;p&gt;$f(\vec{x})=\vec{\omega}^T\vec{x}+b=\sum\limits_{i=1}^m \alpha_iy_ix_i^T\vec{x}+b$&lt;/p&gt;
&lt;p&gt;支持向量机一个极其重要的特性就是：训练完成后，大部分的训练样本不需要保存，最终模型仅与支持向量有关。上式是一个二次规划问题，求解的高效办法是使用SMO。&lt;/p&gt;
&lt;h2 id=&quot;使用SMO求解&quot;&gt;使用SMO求解&lt;/h2&gt;&lt;p&gt;SMO(Sequential Minimal Optimization)基本思路：固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值。&lt;/p&gt;
&lt;p&gt;具体到这个问题，由于存在约束条件$\sum\limits_{i=1}^m\alpha_iy_i=0$，固定$\alpha$之外的值，则$\alpha$值都能导出。所以采用了部分的调整，每次固定两个变量$\alpha_i   \alpha_j$之外的其余参数，之后不断执行下述步骤直到收敛：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;选取一对需要更新的变量$\alpha_i \alpha_j$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;固定其余参数，求解上个小结推导出的式子更新$\alpha_i \alpha_j$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于两个需要更新参数$\alpha_i$和$\alpha_j$的选取，遵循一个规则，使选取的两变量所对应样本之间的间隔最大。&lt;/p&gt;
&lt;h1 id=&quot;核函数&quot;&gt;核函数&lt;/h1&gt;&lt;p&gt;这个部分解决不能线性可分问题。&lt;/p&gt;
&lt;p&gt;这类问题，一般是将样本从原始空间映射到更高维的特征空间，使得样本在高维度空间中线性可分。&lt;/p&gt;
&lt;h2 id=&quot;引出&quot;&gt;引出&lt;/h2&gt;&lt;p&gt;令$\phi(\vec{x})$表示将$\vec{x}$映射后的特征向量，于是在特征空间中划分超平面对应的模型可以表示为：&lt;/p&gt;
&lt;p&gt;$f(\vec{x})=\omega^T \phi(\vec{x})+b$&lt;/p&gt;
&lt;p&gt;类似在线性可分情况下的式子：&lt;/p&gt;
&lt;p&gt;$\min\limits_{\vec{w},b} \frac{\mid\mid \vec{\omega} \mid\mid^2}{2}$&lt;/p&gt;
&lt;p&gt;约束条件：&lt;/p&gt;
&lt;p&gt;$y_i(\vec{\omega}^T \phi(x_i)+b) \geq 1, i=1,2,…,m$&lt;/p&gt;
&lt;p&gt;对偶问题：&lt;/p&gt;
&lt;p&gt;$\max\limits_{\alpha}  \sum\limits_{i=1}^m\alpha_i-\frac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m \alpha_i\alpha_jy_iy_j \phi(x_i)^T\phi(x_j)$&lt;/p&gt;
&lt;p&gt;约束条件：$\sum\limits_{i=1}^m \alpha_i y_i =0$&lt;/p&gt;
&lt;p&gt;关键问题$\phi(x_i)^T\phi(x_j)$是映射到特征空间之后的内积。由于维度太高不易计算，构造这样的函数：&lt;/p&gt;
&lt;p&gt;$\kappa(\vec{x}_i,\vec{x}_j)=\langle \phi(x_i),\phi(x_j) \rangle = \phi(x_i)^T\phi(x_j)$&lt;/p&gt;
&lt;p&gt;原式求解得到：&lt;/p&gt;
&lt;p&gt;$f(\vec{x})=\vec{\omega}^T\phi(\vec{x})+b$&lt;/p&gt;
&lt;p&gt;$=\sum\limits_{i=1}^m\alpha_iy_i\phi(x_i)^T\phi(x)+b$&lt;/p&gt;
&lt;p&gt;$=\sum\limits_{i=1}^m\alpha_iy_i\kappa(x,x_i)+b$&lt;/p&gt;
&lt;p&gt;这里的\kappa就是核函数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro6-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro6-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;核函数组合&quot;&gt;核函数组合&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;$\kappa_1$和$\kappa_2$都是核函数，则对于任意正数$\gamma_1$和$\gamma_2$的线性组合:$\gamma_1\kappa_1 + \gamma_2\kappa_2$&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;核函数的内积： $\kappa_1 \bigotimes \kappa_2(\vec{x},\vec{z})=\kappa_1(\vec{x},\vec{z})\kappa_2(\vec{x},\vec{z})$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对于任意函数$g(x)$，$\kappa(\vec{x},\vec{z})=g(x)\kappa_1(\vec{x},\vec{z})g(z)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;都是核函数&lt;/p&gt;
&lt;h1 id=&quot;软间隔&quot;&gt;软间隔&lt;/h1&gt;&lt;p&gt;为了缓解某些连核函数都无法有效处理的分类问题，需要允许SVM在一些样本上出错，即允许某些样本不满足约束$y_i(\vec{w}^Tx_i+b) \geq 1$，引入了软间隔概念。&lt;/p&gt;
&lt;p&gt;优化目标可以修改为：&lt;/p&gt;
&lt;p&gt;$\min\limits{\vec{\omega},b}\frac{\mid\mid \omega \mid\mid^2}{2}+C\sum\limits_{i=1}^m\ell_{0/1}(y_i(\vec{\omega}^Tx_i+b)-1)$&lt;/p&gt;
&lt;p&gt;其中$C &amp;gt; 0$是一个常数，$\ell_{0/1}$是”0/1损失函数”&lt;/p&gt;
&lt;p&gt;$\ell_{0/1}(z) = 1 if z&amp;lt;0 else 0$&lt;/p&gt;
&lt;p&gt;当C无穷大时，所有样本均需要满足约束，等价于前面小结的式子；C为可数实数，则允许样本不满足约束&lt;/p&gt;
&lt;p&gt;为了使得$\ell$容易求导，通常使用下面几个数学性质较好的式子替代：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro6-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;代入原式，然后使用松弛变量$\xi_i \geq 0$，松弛变量$\xi$替换原来C后面的部分，表示样本不满足约束的程度。&lt;/p&gt;
&lt;p&gt;使用之前介绍的对偶问题求解出最终的答案。&lt;/p&gt;
&lt;h1 id=&quot;支持向量回归&quot;&gt;支持向量回归&lt;/h1&gt;&lt;p&gt;支持向量回归（Support Vector Regression）假设我们能够容忍f(x)与y之间最多有$\epsilon$的误差，即仅当f(x)与y之间差别的绝对值大于$\epsilon$时才算损失。&lt;/p&gt;
&lt;p&gt;相当于以f(x)为中心，构建了一个宽度为$2\epsilon$的间隔带，落在带内的样本是正确的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-ml-intro6-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SVR问题可用式子表示：&lt;/p&gt;
&lt;p&gt;$\min\limits{\vec{\omega},b}\frac{\mid\mid \omega \mid\mid^2}{2}+C\sum\limits_{i=1}^m\ell_{\epsilon}(f(x_i-y_i))$&lt;/p&gt;
&lt;p&gt;其中$\ell_{\epsilon}$：$\ell_{\epsilon} ==0 if \mid z\mid \leq \epsilon else (\mid z \mid - \epsilon)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;引入松弛变量$\xi_{i}$和$\hat{\xi_{i}}$，原式重写为$\min\limits_{\vec{\omega},b,\xi_{i},\hat{\xi_{i}}}\frac{\mid\mid \vec{\omega}\mid\mid^2}{2}+C\sum\limits_{i=1}^m(\xi_{i} + \hat{\xi_{i}})$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;引入拉格朗日乘子，求对偶问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;推导出带有核函数的算式&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;支持向量机（Support Vector Machine）的求解通常是借助凸优化技术。&lt;/p&gt;
&lt;h1 id=&quot;间隔与支持向量&quot;&gt;间隔与支持向量&lt;/h1&gt;&lt;p&gt;给定训练样本集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)},y_i \in (-1,+1 )$  分类学习最基本的思想就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。SVM就是帮助我们寻找到众多划分超平面中最符合的。&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML学习-神经网络</title>
    <link href="http://peihao.space/2017/03/04/ml-intro5/"/>
    <id>http://peihao.space/2017/03/04/ml-intro5/</id>
    <published>2017-03-04T05:18:42.000Z</published>
    <updated>2017-03-03T11:05:09.511Z</updated>
    
    <content type="html">&lt;h1 id=&quot;神经元&quot;&gt;神经元&lt;/h1&gt;&lt;p&gt;Neural Network神经网络：是由具有适应性的简单单元组成的广泛并进行互联的网络，他的组织能够模拟生物神经系统对真实的世界物体做出的交互反应&lt;/p&gt;
&lt;p&gt;神经网络由一个个神经元（neuron）互联组成，神经元可以看作一个处理函数，当这个函数的输入超过某一个阈值，经过处理后单元会向其他单元发送信号。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;单元的输入来自其他单元传递过来的信号，这些信号通过带权重的连接进行传递。当前单元接受来自其余单元的带权重的信号和与阈值比较，符合条件的产生输出。&lt;/p&gt;
&lt;h1 id=&quot;感知机&quot;&gt;感知机&lt;/h1&gt;&lt;p&gt;感知机由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是阈值逻辑单元。&lt;/p&gt;
&lt;p&gt;感知机通过简单的算术组合可以轻易的实现与或非逻辑运算。&lt;/p&gt;
&lt;p&gt;实现逻辑类似于$y=f(\sum_i\omega_ix_i-\theta)$，其中w是权重向量，x是输入信号，$\theta$则是阈值。&lt;/p&gt;
&lt;p&gt;阈值可看作是一个固定输入(x值)为-1的dummy node所对应的连接权重$\omega_{n+1}$，这样权重和阈值就可以统一学习。&lt;/p&gt;
&lt;h2 id=&quot;学习规则&quot;&gt;学习规则&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$\omega_i \leftarrow \omega_i + \Delta \omega_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\Delta \omega_i = \eta(y-\hat{y})x_i$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中$\eta \in (0,1)$称为学习率。若感知机对训练样例(x,y)预测正确，即$\hat{y}=y$，则感知机不改变，否则将根据错误的程度进行权重调整。&lt;/p&gt;
&lt;h2 id=&quot;非线性与多重神经元&quot;&gt;非线性与多重神经元&lt;/h2&gt;&lt;p&gt;要解决非线性可分问题，需考虑多层功能神经元。例如异或问题。&lt;/p&gt;
&lt;p&gt;一般的多层功能神经元都设置在输入层与输出层之间，也称作隐含层。每一层都有独立的激活函数功能神经元。&lt;/p&gt;
&lt;p&gt;功能更强大的神经网络：每层神经元与下一层神经元完全互联，而同层之间不存在连接，也不存在跨层连接。要注意的是，这里的相邻层连接是双向的。这种分类型的神经网络通常被称为”多层前馈神经网络”(multi-layer feedforward neural networks)这种网络的输入层的功能仅仅是接受输入，隐层以及输出层有功能神经元，进行函数处理。&lt;/p&gt;
&lt;h2 id=&quot;学习过程&quot;&gt;学习过程&lt;/h2&gt;&lt;p&gt;神经网络就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值。&lt;/p&gt;
&lt;h1 id=&quot;误差逆传&quot;&gt;误差逆传&lt;/h1&gt;&lt;p&gt;多层神经网络中最著名的学习算法就是BP误差逆传算法（erroe BackPropagation）。&lt;/p&gt;
&lt;p&gt;BP算法可以训练包括多层前馈神经网络、递归神经网络等。&lt;/p&gt;
&lt;p&gt;BP算法的目标是要最小化训练集D上的累计误差。作为一个迭代类型的算法，，迭代的每一轮采用广义的感机学习规则对参数进行更新估计。其中学习率$\eta \in (0,1)$控制着算法中每次迭代的更新步长，$\eta$太大容易造成迭代的震荡，太小则会影响收敛速度，&lt;/p&gt;
&lt;h2 id=&quot;算法流程&quot;&gt;算法流程&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算输出层的无擦汗，将误差逆向传播至隐层神经元&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据隐层神经元的误差对连接券和阈值进行调整&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;迭代，直到达到某个停止条件，如训练误差小于某个数值&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;对过拟合的策略&quot;&gt;对过拟合的策略&lt;/h2&gt;&lt;p&gt;由于BP算法强大的学习能力，经常会在训练集上过度学习造成过拟合现象，反而在测试集上的表现不好。通常可以采用”早停”策略：将数据集分为训练集和验证集，训练集用来计算梯度、更新连接权以及阈值，验证集则用来估计误差。训练集误差降低但验证集误差升高时，停止训练，同时返回具有最小验证集误差的连接权和阈值。&lt;/p&gt;
&lt;h2 id=&quot;全局与局部&quot;&gt;全局与局部&lt;/h2&gt;&lt;p&gt;用E表示神经网络在训练集上的误差，则E是关于连接权w和阈值$\theta$的函数，此时神经网络的训练过程可看作是一个参数的寻优过程，即在参数空间中找一组最优参数使得E最小。&lt;/p&gt;
&lt;p&gt;直观的得看，局部最小解(local minimum)是参数空间中的某个点，它相邻的误差函数值均不小于该点的函数值；&lt;/p&gt;
&lt;p&gt;全局最小解（global minimum）是指参数空间中所有的点误差函数值都不小于该店的误差函数值。&lt;/p&gt;
&lt;p&gt;显然有全局最小解必然是局部最小解，反之未必。参数空间中梯度为零的点，其误差值小于临点的误差函数值，称为局部最小。&lt;/p&gt;
&lt;h2 id=&quot;搜索方式&quot;&gt;搜索方式&lt;/h2&gt;&lt;p&gt;根据梯度的值决定参数最优搜索的方向是最广泛的办法。例如负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，则已达到局部极小。&lt;/p&gt;
&lt;p&gt;如果只有一个局部最小点，则为全局最小解。从局部最优找到全局最优的办法就是要从局部最优中挑出来，继续搜索下去。因为搜索到局部最优后，梯度为零，会造成搜索停滞。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;取多组不同参数值初始化多个神经网络，取其中误差最小的解作为最终的参数。相当于从不同的初始值开始搜索猜测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用模拟退火法（在blog之前的文章中有介绍）。模拟退退火有一定的概率接受比当前解差的解，有助于跳出局部极小。而迭代过程，接受次优解的概率逐渐降低，从而保证了算法的稳定性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;随机梯度下降&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;其余常见的神经网络&quot;&gt;其余常见的神经网络&lt;/h1&gt;&lt;h2 id=&quot;RBF网络&quot;&gt;RBF网络&lt;/h2&gt;&lt;p&gt;单隐层前馈神经网络的一种，输出层是对隐层神经元还输出的线性组合。假定输入为d维向量$\vec{x}$输出为实值，可表示为：&lt;/p&gt;
&lt;p&gt;$\phi(x)=\sum_{i=1}^{q}\omega_i\rho(x,c_i)$&lt;/p&gt;
&lt;p&gt;q为隐层神经元个数，$c_i$和$\omega_i$分别是第i个隐层神经元对应的中心和权重。$\rho(x,c_i)$是径向基函数，是某种沿径向对称的标量函数。通常定义为样本x到数据中心&lt;br&gt;$c_i$之间的欧式距离的单调函数。&lt;/p&gt;
&lt;h2 id=&quot;ART网络&quot;&gt;ART网络&lt;/h2&gt;&lt;p&gt;竞争学习(competitive learning)是神经网络中常用的无监督学习策略。&lt;/p&gt;
&lt;p&gt;网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元状态被抑制。&lt;/p&gt;
&lt;h2 id=&quot;SOM网络&quot;&gt;SOM网络&lt;/h2&gt;&lt;p&gt;自组织映射，是一种竞争学习型的无监督神经网络。能将高位输入数据降维，同时保持输入数据的拓扑结构。即将高维空间中相似的样本点映射到网络输出层的邻近神经元。&lt;/p&gt;
&lt;p&gt;在接受一个训练样本之后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元称为最佳匹配单元。然后调整最佳匹配单元及邻近神经元的权向量，使得这些权向量与当前输入样本距离缩小。这个过程不断迭代，知道收敛。&lt;/p&gt;
&lt;h2 id=&quot;级连相关&quot;&gt;级连相关&lt;/h2&gt;&lt;p&gt;级联： 建立层次连接的层级结构。开始训练时，只有几本的输入输出层，随着训练的进行，新的隐层神经元逐渐加入。&lt;/p&gt;
&lt;p&gt;相关： 通过最大化新神经元的输出与网络误差之间的相关性来训练相关的参数。&lt;/p&gt;
&lt;h2 id=&quot;Elman&quot;&gt;Elman&lt;/h2&gt;&lt;p&gt;Elman网络是递归神经网络（recurrent neural networks）的一种。递归神经网络可以让一些神经元的输出反馈作为输入信号。&lt;/p&gt;
&lt;h1 id=&quot;深度学习&quot;&gt;深度学习&lt;/h1&gt;&lt;p&gt;深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。&lt;/p&gt;
&lt;p&gt;同机器学习方法一样，深度机器学习方法也有监督学习与无监督学习之分．不同的学习框架下建立的学习模型很是不同．例如，卷积神经网络（Convolutional neural networks，简称CNNs）就是一种深度的监督学习下的机器学习模型，而深度置信网（Deep Belief Nets，简称DBNs）就是一种无监督学习下的机器学习模型。&lt;/p&gt;
&lt;p&gt;DBN每次训练一层隐节点，训练时将上一层隐节点的输出作为下一层隐节点的输入。整个网络完成后，再进程微调。&lt;/p&gt;
&lt;p&gt;CNN节省训练开销的策略是”权共享”，让一组神经元使用相同的连接权。&lt;/p&gt;
&lt;p&gt;无论是DBN还是CNN，其多隐层堆叠，每层对上层的处理机制，可以看作是对输入信号逐层加工，从而把初始的与输出目标联系不密切的输入转换呈密切。通过多层处理，逐渐的将初始的低层特征转转成高层特征表示，用简单模型完成复杂的分类任务。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;神经元&quot;&gt;神经元&lt;/h1&gt;&lt;p&gt;Neural Network神经网络：是由具有适应性的简单单元组成的广泛并进行互联的网络，他的组织能够模拟生物神经系统对真实的世界物体做出的交互反应&lt;/p&gt;
&lt;p&gt;神经网络由一个个神经元（neuron）互联组成，神经元可以看作一个处理函数，当这个函数的输入超过某一个阈值，经过处理后单元会向其他单元发送信号。&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML学习-决策树</title>
    <link href="http://peihao.space/2017/03/04/ml-intro4/"/>
    <id>http://peihao.space/2017/03/04/ml-intro4/</id>
    <published>2017-03-04T05:18:42.000Z</published>
    <updated>2017-03-05T04:33:47.459Z</updated>
    
    <content type="html">&lt;p&gt;决策树（decision tree）是一种常见的机器学习方法。目标是生成一个具有强泛化能力的（对未遇到的样本有很高的适应性）树形结构。&lt;/p&gt;
&lt;p&gt;决策树包含一个根结点，若干内部结点以及若干叶子结点。叶子结点对应了确定的判断结果，其余的结点都是一个个的属性测试，属性测试分成的不同情况就是当前结点的子结点。决策树是一个递归的过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro4-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;决策树生成学习的过程中，最重要的就是如何选择最优的划分属性。一般而言，随着划分过程的不断进行，决策树分支结点包含的样本应该尽可能的靠近。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;信息熵&quot;&gt;信息熵&lt;/h1&gt;&lt;p&gt;信息熵（information entropy）是度量样本集合纯度最常用的一种指标。假设当前样本集合D中第k类样本比例为$p_{k}$(k=1,2,3,…,|y|)$，则D的信息熵为&lt;/p&gt;
&lt;p&gt;$Ent(D)=-\sum\limits_{k=1}^{|y|}p_{k=1}log_2p_{k}$&lt;/p&gt;
&lt;p&gt;Ent(D)的值越小，则D的纯度越高.&lt;/p&gt;
&lt;h2 id=&quot;信息增益&quot;&gt;信息增益&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;假定离散属性a有V个可能的取指{$a^1,a^2,…,a^V$},若使用a来对样本集合D进行划分，则会产生V个分支结点，其中第v个节点包含了D中所有在属性a上取指为$a^V$的样本，记为$D^v$。根据上师酸楚$D^v$的信息熵，然后考虑不同分支结点包含的样本数目不同，给分支结点赋予权重$|D^v|/|D|$，即样本越多，分支结点的影响越大，此时使用一个统一的计算公式算得属性啊对样本集D进行划分得到的”信息增益”（information gain）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^{V}\frac{D^v}{D}Ent(D^v)$&lt;/p&gt;
&lt;p&gt;一般而言，信息增益越大，意味着使用属性啊来划分所获得的纯度提升越大。所以，可以使用信息增益Gain作为属性划分的指标。&lt;/p&gt;
&lt;h2 id=&quot;基尼指数&quot;&gt;基尼指数&lt;/h2&gt;&lt;p&gt;CART决策树使用“基尼指数”选择划分属性。&lt;/p&gt;
&lt;p&gt;数据集D的纯度可用基尼值来度量：&lt;/p&gt;
&lt;p&gt;$Gini(D)=\sum\limits_k^{|y|}\sum\limits_{j\neq k}p_kp_j$&lt;/p&gt;
&lt;p&gt;$=1-\sum\limits_k^{|y|}p_{k}^2$&lt;/p&gt;
&lt;p&gt;直观来讲，Gini（D）反映了从数据D中随机抽取两个样本，类别标记不一致的概率。因此Gini（D）越小，数据集D的纯度越高。&lt;/p&gt;
&lt;p&gt;属性a的基尼指数定义为：&lt;/p&gt;
&lt;p&gt;$Gini_index(D,a)=\sum\limits_v^{V}\frac{|D^v|}{|D|}Gini(D^v)$&lt;/p&gt;
&lt;p&gt;计算属性集合A所有属性的基尼指数，取最小值得属性作为最优划分&lt;/p&gt;
&lt;h1 id=&quot;剪枝处理&quot;&gt;剪枝处理&lt;/h1&gt;&lt;p&gt;剪枝（pruning）是决策树学习算法中对付“过拟合”的主要手段。过拟合的定义可以参考系列的第一篇。决策树算法经常会由于过拟合造成分支过多，在泛化性能上造成影响。&lt;/p&gt;
&lt;p&gt;决策树剪枝基本策略有两种：预剪枝（prepruning）和后剪枝（postpruning）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;预剪枝：在生成决策树的过程中，对每个结点在划分前进行估计，若当前划分不能对决策树带来泛化性能上的提升，则停止划分并将当前节点标记为叶结点。（训练开销短，保留子树少，易造成欠拟合）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;后剪枝：决策树生成后自底向上的对内部结点进行考察。如果当前结点子树替换成叶子结点可以在泛化能力上得到提升，则将子树替换。（训练开销长，保留子树多，欠拟合风险小）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里使用集精度作为泛化能力的指标。&lt;/p&gt;
&lt;p&gt;需要注意的是，虽然在当时结点的集精度没有下降（也就是泛化能力未降低），但是很可能在之后进一步的展开，有欠拟合的风险。&lt;/p&gt;
&lt;h1 id=&quot;连续值与缺省值处理&quot;&gt;连续值与缺省值处理&lt;/h1&gt;&lt;h2 id=&quot;连续&quot;&gt;连续&lt;/h2&gt;&lt;p&gt;由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对结点进行划分。此时可以使用连续属性离散化技术。&lt;/p&gt;
&lt;p&gt;最简单的策略是采用二分法(bi-partition)对连续属性进行处理。&lt;/p&gt;
&lt;p&gt;给定样本集D和连续属性a,假定在D上出现了n个不同的取值，将这些值从小到大进行排序，记为${a^1,a^2,…,a^n}$.基于划分点t可将D分为子集$D_t^-$和$D_t^+$其中$D_t^-$包含那些在属性上取值不大于t的样本，而$D_t^+$则包含那些在属性a上取值大于t的样本。&lt;/p&gt;
&lt;p&gt;对相邻的属性取值[$a^i，a^{i+1}$]来说结果是相同的，因此在此区间都取值$\frac{a^i+a^{i+1}}{2}$&lt;/p&gt;
&lt;p&gt;对n-1个区间取指作为划分点，分别测试选取最优。&lt;/p&gt;
&lt;p&gt;$Gain(D,a)=\max\limits_{t\in T_a}Gain(D,a,t)$&lt;/p&gt;
&lt;p&gt;$=\max\limits_{t\in T_a}Ent(D)-\sum\limits_{\lambda \in {-,+}}\frac{\mid D_t^{\lambda}\mid}{\mid D \mid}Ent(D_t^{\lambda})$&lt;/p&gt;
&lt;p&gt;Gain(D,a,t)是样本集D基于划分点t二分后的信息增益，我们可以使用Gain(D,a,t)最大化的划分点&lt;/p&gt;
&lt;h2 id=&quot;缺省&quot;&gt;缺省&lt;/h2&gt;&lt;p&gt;嗯，本部分也暂时缺省&lt;/p&gt;
&lt;h1 id=&quot;多变量决策树&quot;&gt;多变量决策树&lt;/h1&gt;&lt;p&gt;我们把每个属性视为坐标空间的一个坐标轴，则d个属性描述的样本就对赢了d维空间中的一个数据点。对样本的分类也就是在这个坐标空间中寻找不同样本之间的分类边界。&lt;/p&gt;
&lt;p&gt;决策树形成的分类边界有一个明显的特点：轴平行（axis-parallel），即分类边界由若干个与坐标轴平行的分段组成。&lt;/p&gt;
&lt;p&gt;使用斜划分边界可以将决策树模型简化。此时，非叶节点不仅是针对某个属性，而是属性的线性组合进行测试。与传统的决策树不同，多变量决策树试图建立一个合适的线性分类器。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;决策树（decision tree）是一种常见的机器学习方法。目标是生成一个具有强泛化能力的（对未遇到的样本有很高的适应性）树形结构。&lt;/p&gt;
&lt;p&gt;决策树包含一个根结点，若干内部结点以及若干叶子结点。叶子结点对应了确定的判断结果，其余的结点都是一个个的属性测试，属性测试分成的不同情况就是当前结点的子结点。决策树是一个递归的过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro4-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;决策树生成学习的过程中，最重要的就是如何选择最优的划分属性。一般而言，随着划分过程的不断进行，决策树分支结点包含的样本应该尽可能的靠近。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>线性模型</title>
    <link href="http://peihao.space/2017/03/03/ml-intro3/"/>
    <id>http://peihao.space/2017/03/03/ml-intro3/</id>
    <published>2017-03-03T04:05:50.000Z</published>
    <updated>2017-03-05T03:11:01.276Z</updated>
    
    <content type="html">&lt;p&gt;示例有d个属性$(x_1 ; x_2 ; x_3… ; x_d)$描述，线性模型试图学得一个通过属性的线性组合来进行预测的函数。&lt;/p&gt;
&lt;p&gt;$f(x)=w_1x_1+w_2x_2+…+w_dx_d+b$，w和b确定后，模型就确定了。w直观表达了各属性在预测中的重要性。&lt;/p&gt;
&lt;h1 id=&quot;线性回归&quot;&gt;线性回归&lt;/h1&gt;&lt;p&gt;线性回归试图学得$f(x_i)=wx_i+b$，使得$f(x_i) \approx y_i$&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;确定w和b的关键是衡量f(x)与y之间的关系。我们可以通过使用均方误差最小化作为衡量标准。&lt;/p&gt;
&lt;h1 id=&quot;对数几率回归&quot;&gt;对数几率回归&lt;/h1&gt;&lt;p&gt;对数几率回归实际上针对的是分类任务，将分类任务的真实标记y和回归模型的预测值联系起来。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro3-1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;分类任务，通过类阶跃函数（阶跃函数不连续），把连续的值近似的分成两类（0或1），并且在坐标原点处变得很陡。&lt;/p&gt;
&lt;h1 id=&quot;线性判别分析&quot;&gt;线性判别分析&lt;/h1&gt;&lt;p&gt;Linear Discriminant Analysis是一种经典的线性学习方法：给定训练样例集，设法将样例投射到一条直线上，1. 使得同样样例的投影点尽可能的接近，2. 异类样例投射点尽可能远。对测试集预测时，将对象投射到直线上预测。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/ml-intro3-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;要实现上面的两个目标，分别考虑：对于同类样本投射点接近，计算同类样本点的均值方差A；异类点则考虑两个样本簇中心点的距离差最大&lt;code&gt;|xa-xb|²&lt;/code&gt;，然后使用单一的数值，J=A/B，使J尽可能的大。&lt;/p&gt;
&lt;h1 id=&quot;多分类策略&quot;&gt;多分类策略&lt;/h1&gt;&lt;p&gt;这里简单介绍通过二分策略实现多分类策略，常用的有三种方法，核心是拆解的策略区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;一对一（OvO）：任意两个对象亮亮配对，产生N*(N-1)/2个预测结果，在这些结果中投票，预测结果最多的视为最终结果&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一对多（OvR）：每次从训练集中挑选一个不重复的样例作为正例，其余N-1个样例作为反例，进行N次训练。如果某次训练的结果预测为正类，此次训练的样例即为结果；若有多个样例被预测为正例，考虑事先设置的置信度，选择置信度最大的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;多对多（MvM）：每次将若干类作为正类，若干类作为反类。这里提供一种常用的MvM拆分模式，就错输出吗ECOC。ECOC进行M次的划分，也就是M次分类器。分别对测试样本进行预测，这些预测标记组成一个编码，然后编码与每个类别各自的编码进行比较，返回其中距离和最小的作为最终预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;示例有d个属性$(x_1 ; x_2 ; x_3… ; x_d)$描述，线性模型试图学得一个通过属性的线性组合来进行预测的函数。&lt;/p&gt;
&lt;p&gt;$f(x)=w_1x_1+w_2x_2+…+w_dx_d+b$，w和b确定后，模型就确定了。w直观表达了各属性在预测中的重要性。&lt;/p&gt;
&lt;h1 id=&quot;线性回归&quot;&gt;线性回归&lt;/h1&gt;&lt;p&gt;线性回归试图学得$f(x_i)=wx_i+b$，使得$f(x_i) \approx y_i$&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>模型评估与选择</title>
    <link href="http://peihao.space/2017/03/01/ml-intro2/"/>
    <id>http://peihao.space/2017/03/01/ml-intro2/</id>
    <published>2017-03-01T04:05:50.000Z</published>
    <updated>2017-03-01T02:54:56.183Z</updated>
    
    <content type="html">&lt;h1 id=&quot;误差与拟合&quot;&gt;误差与拟合&lt;/h1&gt;&lt;h2 id=&quot;误差&quot;&gt;误差&lt;/h2&gt;&lt;p&gt;学习器的实际预测输出与样本的真是输出之间的差异称为”误差”&lt;/p&gt;
&lt;p&gt;学习器在训练集上的误差叫”训练误差”&lt;/p&gt;
&lt;p&gt;学习器在新样本的误差叫”泛化误差”（重要）&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;拟合&quot;&gt;拟合&lt;/h2&gt;&lt;p&gt;我们实际希望的，是在新样本上能表现得很好的学习器.为了达到这个 目的，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样才能在遇到新样本时做出正确的判别。&lt;/p&gt;
&lt;p&gt;然而，当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的普适性质，这样就会导致泛化性能下降。这种现象在机器学习中称为“过合”（overfitting).&lt;/p&gt;
&lt;p&gt;与“过拟合”相对的是“欠拟合”（underfitting),这是指对训练样本的一般性质尚未学好。&lt;/p&gt;
&lt;h1 id=&quot;评估方法&quot;&gt;评估方法&lt;/h1&gt;&lt;h2 id=&quot;留出法&quot;&gt;留出法&lt;/h2&gt;&lt;p&gt;将整体数据划分为两个互斥的数据集合，分别作为测试集与训练集。注意测试集与训练集要尽量保证层次、类别上的相似。可以进行多次留出法，然后取误差的平均值。一般取2/3或者3/4的数据作为训练集。&lt;/p&gt;
&lt;h2 id=&quot;交叉验证&quot;&gt;交叉验证&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;同样是将数据集合互斥划分，这里互斥的划分为k份，相互之间保持层次分布的平衡。&lt;/li&gt;
&lt;li&gt;每次使用k-1份数据作为训练集，余下的作为预测集&lt;/li&gt;
&lt;li&gt;进行k次实验，取平均误差值&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;自助法&quot;&gt;自助法&lt;/h2&gt;&lt;p&gt;上述的两种方法有一个共同的缺陷就是都把训练集合与整体的样本集合规模有偏差，而且由于每次的偏差不同，造成无法预料的影响。&lt;/p&gt;
&lt;p&gt;自助法：给定包含m个样本的数据集D，对他进行采样产生数据集d。每次随机的从D中跳一个样本复制到d中，进行m次，生成了包含m个样本的数据集d。显然，d中可能包含多个重复的样本数据。经过计算，m取无限大时，d中大概有D的63%数据。&lt;/p&gt;
&lt;p&gt;之后我们使用d作为训练样本，而D\d作为测试集合。这样，实际的训练与测试都使用了m个样本，且保证了D\d有3成多的非训练样本。&lt;/p&gt;
&lt;h1 id=&quot;性能度量&quot;&gt;性能度量&lt;/h1&gt;&lt;p&gt;这里主要介绍两个属性：P（查准率precision）与R（查全率recall）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;查全率＝（检索出的相关信息量/系统中的相关信息总量）*100%  用户感兴趣的信息中有多少被检索出来&lt;/li&gt;
&lt;li&gt;查准率＝（检索出的相关信息量/检索出的信息总量）*100%  检索出的信息有多少比例是用户感兴趣的&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;查全率是衡量检索系统和检索者检出相关信息的能力，查准率是衡量检索系统和检索者拒绝非相关信息的能力。&lt;/p&gt;
&lt;p&gt;实验证明，在查全率和查准率之间存在着相反的相互依赖关系–如果提高输出的查全率，就会降低其查准率，反之亦然。&lt;/p&gt;
&lt;p&gt;以P作为纵轴，R作为横轴作图，简称”P-R曲线”。若一个学习方法的P-R曲线完全包括另外一个，则断言此学习方法更优秀。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;误差与拟合&quot;&gt;误差与拟合&lt;/h1&gt;&lt;h2 id=&quot;误差&quot;&gt;误差&lt;/h2&gt;&lt;p&gt;学习器的实际预测输出与样本的真是输出之间的差异称为”误差”&lt;/p&gt;
&lt;p&gt;学习器在训练集上的误差叫”训练误差”&lt;/p&gt;
&lt;p&gt;学习器在新样本的误差叫”泛化误差”（重要）&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML相关术语</title>
    <link href="http://peihao.space/2017/02/28/ml-intro/"/>
    <id>http://peihao.space/2017/02/28/ml-intro/</id>
    <published>2017-02-28T03:00:13.000Z</published>
    <updated>2017-03-09T15:15:36.909Z</updated>
    
    <content type="html">&lt;p&gt;机器学习：研究如何通过计算的手段，利用经验来改善系统自身的性能。&lt;/p&gt;
&lt;p&gt;计算机系统中，“经验”通常以“data”形式存在，所以机器学习从数据中产生模型（model）的算法，称为学习算法。模型，即从数据中学到的结果。&lt;/p&gt;
&lt;p&gt;通过向学习算法中输入已有的数据，算法产生模型，面对新的case时，模型就可以通过case的特性进行判断。&lt;/p&gt;
&lt;p&gt;一组数据的集合称为数据集，其中每条记录是关于一个事件或对象的描述，称为”示例”（instance）或者”样本”（sample）。&lt;/p&gt;
&lt;p&gt;描述中不同的性质称为属性。通过属性做成的坐标轴称为属性空间、样本空间或者输入空间。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;在属性空间中，每个描述都与一个点对应，所以我们也把一个描述称作是”特征向量”。&lt;/p&gt;
&lt;p&gt;从数据中学得模型的过程称为”学习”或者”训练”，这个过程通过执行某个学习算法完成。训练样本组成的集合称为”训练集”&lt;/p&gt;
&lt;p&gt;训练样例的结果信息称为“样例”。 如果需要预测的是离散值，例如（1、2、3、4、5），称为分类预测（classification）。特别的，离散值只有两个的情况称为二分类（binary classification），多个离散值称为多分类。如果需要预测的结果范围是一个连续值，或者是不可数结果，此时的学习任务称为回归任务（regression）。&lt;/p&gt;
&lt;p&gt;聚类：将训练集中的西瓜分为若干组，每组称为一个”簇”；簇是自动形成的，对应一些潜在的概念划分，这些潜在的概念划分我们事先不知道，而且学习过程中使用的训练样本通常不用有标记信息（提前设定的结果）&lt;/p&gt;
&lt;p&gt;训练样本有标记信息的学习任务称为监督学习（supervised learning），否则称为无监督学习（unsupervised learning）。分类回归是监督学习的代表，聚类（clustering）是后者的代表。&lt;/p&gt;
&lt;p&gt;泛化（generalization）：学得模型适用于新样本的能力。&lt;/p&gt;
&lt;h1 id=&quot;归纳偏好&quot;&gt;归纳偏好&lt;/h1&gt;&lt;p&gt;由于训练样本的数据量无法代表整个样本空间，可能会存在这种情况：预测样本可以匹配到多个训练样本结果，而这些不同的训练结果有不同的输出。表现为分类中属不同类。&lt;/p&gt;
&lt;p&gt;此时无法通过匹配的手段确定到底使用哪个训练样本结果，学习算法的”偏好”就很重要了。通过提前设置的偏好，在遇到这种情况时候，算法会自动根据偏好选择合适的预测结果。称为”归纳偏好”（inductive bias）&lt;/p&gt;
&lt;h2 id=&quot;奥卡姆剃刀&quot;&gt;奥卡姆剃刀&lt;/h2&gt;&lt;p&gt;奥卡姆剃刀是一种常用的、自然科学研究中最基本的原则，即”若有多个假设与观察一直，则选择最简单的那个”。&lt;/p&gt;
&lt;p&gt;如无必要，勿增实体&lt;/p&gt;
&lt;h2 id=&quot;NFL定理&quot;&gt;NFL定理&lt;/h2&gt;&lt;p&gt;所有问题同等重要的前提下，任意两个学习算法的期望性能是相同的。就是说误差率是相同的。NFL定理的重要定义是要我们认识到，脱离具体问题，空泛的谈论”什么学习算法更好没有意义”。学习算法自身的归纳偏好与问题是否匹配，往往起到决定性作用。&lt;/p&gt;
&lt;h1 id=&quot;历史进程&quot;&gt;历史进程&lt;/h1&gt;&lt;h2 id=&quot;机械学习&quot;&gt;机械学习&lt;/h2&gt;&lt;p&gt;将所有样例记住，并在需要预测的时候拿出，实际上是一种检索方法，没有涉及到学习。&lt;/p&gt;
&lt;h2 id=&quot;符号主义学习&quot;&gt;符号主义学习&lt;/h2&gt;&lt;p&gt;这个阶段，代表的学习方法有决策树和基于逻辑学习。&lt;/p&gt;
&lt;h2 id=&quot;基于连接主义&quot;&gt;基于连接主义&lt;/h2&gt;&lt;p&gt;主要是神经网络学习，BP学习方法大放异彩。&lt;/p&gt;
&lt;h2 id=&quot;基于统计的学习&quot;&gt;基于统计的学习&lt;/h2&gt;&lt;p&gt;SVM支持向量机的提出&lt;/p&gt;
&lt;h1 id=&quot;DM与ML&quot;&gt;DM与ML&lt;/h1&gt;&lt;p&gt;数据挖掘（Data Mining）是从海量的数据中发掘知识，主要包括两个支撑技术：数据库管理和机器学习。&lt;/p&gt;
&lt;p&gt;数据库领域为数据挖掘提供数据管理技术，而ML和统计学为DM提供数据分析技术。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习：研究如何通过计算的手段，利用经验来改善系统自身的性能。&lt;/p&gt;
&lt;p&gt;计算机系统中，“经验”通常以“data”形式存在，所以机器学习从数据中产生模型（model）的算法，称为学习算法。模型，即从数据中学到的结果。&lt;/p&gt;
&lt;p&gt;通过向学习算法中输入已有的数据，算法产生模型，面对新的case时，模型就可以通过case的特性进行判断。&lt;/p&gt;
&lt;p&gt;一组数据的集合称为数据集，其中每条记录是关于一个事件或对象的描述，称为”示例”（instance）或者”样本”（sample）。&lt;/p&gt;
&lt;p&gt;描述中不同的性质称为属性。通过属性做成的坐标轴称为属性空间、样本空间或者输入空间。&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>全球通史笔记</title>
    <link href="http://peihao.space/2017/02/13/global-history/"/>
    <id>http://peihao.space/2017/02/13/global-history/</id>
    <published>2017-02-13T04:27:24.000Z</published>
    <updated>2017-02-13T04:46:52.796Z</updated>
    
    <content type="html">&lt;p&gt;作者说过要“站在月球上来审视人类史”， 这是一本时间跨度长达400万年的历史巨著，所以他给我们的更多的应该是整个地球无尽岁月的点点缩影，让我们按照地理上的分布对不同地区不同时间不同文明有所了解。他更多承担的应该是兴趣入门。“历史的今天”，随时把历史上的重大变故跟当今世界的现状联系在一起，提醒着我们认清所生活的现实世界与历史的内在联系，从而使我们的思想能跨越时空的限制，在历史与现实的两个时空里驰骋，甚至由此产生自己出对历史事件的联想与对比，产生出自己的思想的火花与创作的冲动。由此让阅读历史，成为一种乐趣，成了一个对历史和现实两个世界的疑问同时不断探询和解答的过程。当我们对某个点感兴趣了，自己去查史料扩展自己的视野。匆匆读完一遍，随手记下感想。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;《全球通史》之前的几个版本将整本书分为两个大的部分，1500年以前的世界，与1500年以前的世界，千余页的内容显然是无法将地球数十亿年数亿平方公里的人物历史囊括，作者在每个大的章节最后部分都会单独开一章：历史对今天的启示，细细想来，作者实际上是通过对古典文明的灭亡来写古典文明的局限性，古典文明为什么无法像现代一样，出现科技的快速进步？前言推荐中有这样一段话很好的诠释了整本书的内容、风格以及目的：“这里有人类的起源，文明的嬗变，有帝国的更迭，宗教的扩散；有对欧亚大陆诸古代文明和古典文明不同命运的宏观思考，也有对人性善恶本质的哲学分析，对文明是“诅咒”还是“福音”的辨证评价，也有对世界愈加两级分化的人道关怀，对人类历史上诸多灾难的渊源——社会的变革总是滞后于技术变革——的忧虑与警示，不同于那种把自己的观点和观念强加给读者的历史学作品，这本书平心静气，娓娓道来，没有教育人的口吻，却把读者引入到了一种求索的境界，让你不由自主地手不释卷。”作者通过不断的推进，向我们展示了自己严密的逻辑思维、理性分析以及作为一个史学家回首历史的独特视角。&lt;/p&gt;
&lt;p&gt;说一点内容性的东西吧，最开始的部分，人类（用《人类简史》上的术语或者说是“智人”）在史前时代，从黑暗中摸索，逐渐进步到会简单思考、群居性、使用工具，可能需要数万年数十万年甚至数百万年，人类从最初的食物采集者身份转变到了食物的生产者，数百万年的时间在史书上用我们现在的视角仅仅数千字就能概括完成，而随着文明的出现，思维的扩展，科技科学的出现，数千字的内容可能只能叙述历史的数千年、数百年、甚至到现在，每年每天都会有许多事情发生。文明的进步在某一层面上来讲也可以这样来看，文明社会遂能使知识不断累积并代代相传，自由与温饱让人们富有创造力，知识加上创造力推动社会的进一步发展。&lt;/p&gt;
&lt;p&gt;3–6 世纪，古典文明陷落。西方古典由于地理因素最受蹂躏（日耳曼人、匈奴人、穆斯林、马札尔人、维京人）；印度南方以及中国南方由于距离游牧民族蛮族地理较远，逃过一劫，两大文明的北部虽然遭受了蛮族的入侵，但是很快入侵的蛮族由于当地领先的文明被同化，同时中国原来北方的民族向南迁移，中国南部地区开始汉化。波斯帝国以及拜占庭帝国强大的军事实力完好的度过了此次入侵；盛极一时的古典西方文明，首当其冲的收到了强有力的袭击，彻底消弭。但也正是这次的毁灭，产生出了新的更有活力的文明，作者在文中对这个观点多次的提起。&lt;/p&gt;
&lt;p&gt;宋朝期间，中国人在造船业和航海业上取得巨大进步，12世纪末，开始取代穆斯林在东亚和东南亚的海上优势。蒙古人建立元朝（1279-1368年）后，中国的船只体积最大，装备最佳；商人遍布东南亚及印度各港口这一时期，中国在世界经济中居主导地位。进口商品除细纹棉织品外，还有中亚的皮革、马匹以及南亚的优质木材、玉石、香料和象牙等原材料。而出口商品，除矿石外，还有书、画，尤其是瓷器、丝绸等产品。明朝（1368－1644年），中国的航海活动达到极盛。&lt;br&gt;中世纪时期，中国人在欧亚大陆的交流中，通常是捐献者，而非接受者。早些时候的情况可能相反，古典时期，美索不达米亚的车轮、辘轱和滑轮，埃及的握杆和曲柄；波斯的风车和小亚细亚的炼铁等，从各自的发源地向四面八方传播。但在公元后的14个世纪中，我们国家则是技术革新的伟大中心，向欧亚大陆算他地区传播了许多发明。像三大发明，培根这样评价道：我们应该注意到这些发明的力量、功效和结果。它们是：印刷术、火药和磁铁。因为这三大发明在文学方面，在战争方面，在航海方面，改变了整个世界许多事物的面貌和状态、并由此产生无数变化，以致似乎没有任何帝国、任何派别、任何星球，能比这些技术发明对人类事务产生更大的动力和影响。&lt;/p&gt;
&lt;p&gt;然而我们可以看到，文明的高度发达在历史上并没有去的长久的繁荣。中世纪的世界，中国是世界上最发的区域，我们有着最美的文化、领先的工艺技术、繁盛的商业以及完善的封建官僚技术，但同样造成了我们的固步自封的思想。当我们发现西方国家之后，上述的优势让我们对他们的文化以及潜力产生轻视，认为这些家伙没什么好学的，但是西方国际却渴望从我们这里学习，落后就要挨打这条定律其实根植于人与人、国与国之间。&lt;/p&gt;
&lt;p&gt;人类学家弗朗兹•博亚兹曾经说过，“人类的历史证明，一个社会集团，其文化的进步往往取决于它是否有机会吸取邻近社会集团的经验。一个社会集团所获得的种种发现可以传给其他社会集团；彼此之间的交流愈多样化，相互学习的机会也就愈多。大体上，文化最原始的部落也就是那些长期与世隔绝的部落，因而，它们不能从邻近部落所取得的文化成就中获得好处”。这样，虽然我们有了可以吸取他人经验的条件，却失去了文明进步的进取心。乾隆皇帝曾在答复1793年英国国王乔治三世要求建立外交和贸易关系时，对我们的态度作出了最好的解释：“在统治这个广阔的世界时，我只考虑一个目标，即维持一个完善的统治，履行国家的职责：奇特、昂贵的东西不会引起我的兴趣。……正如您的大使能亲眼看到的那样，我们拥有一切东西。我根本不看重奇特或精巧的物品，因而，不需要贵国的产品。”&lt;/p&gt;
&lt;p&gt;古代中国的这种情况在现代人类学家看来可以用一个词解释：遏制领先法则。在转变时期起先最发达和最成功的这回要想改变和保持其领先地位将是最困难的。相反，落后和较不成功的社会则可能更能适应变化，并在转变中逐渐处于领先地位。而现在，随着国家逐渐的发展，韬光养晦、厚积薄发依然经常被国家提到，正是应了一句古话：以史为鉴可以知兴替。&lt;br&gt;下面是一些零散的读书段落小记：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;生活、生长的先天因素既是保护层，又是枷锁。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;生产的革命促进生产力的解放，进而引起生产关系的转变，上层建筑的破、立，这是新事物发展阶段；进入相持阶段，一切慢慢固化，产生既得利益集团；最后是革新阶段，在没有的生产革命时，重新建立原有生产力与生产关系的格局；在有外来新的生产革命的涌入时，通过大的变革形成各自新的特色。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;民族性格与人的性格形成差不多，都是由生活环境决定的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;劳动创造价值，但价值分配却是个复杂问题，主要在于人解决了生存问题、有了高层次生活的感觉之后，上层的人会尝试不劳而获、巧取豪夺，结果会必然造成原有社会的崩溃，这不是由于集权中央不控制、也不是由于既得利益的上层人没有节制，而是由于市场经济自我发展的特点决定的。在市场经济中，劳动创造出了生活必需之外的价值，以闲散资金的形式来表现，这种闲散资金本能的向赚钱行业集中，即向周期短、利润高、少费力的行业投资，最终会伤害生产力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;宗教的产生之初是被剥削阶级组织起来的工具，之后会在斗争、妥协中被统治集团有条件的吸收、篡改，由粘合剂、兴奋剂变成鸦片，由正视现实变成睡梦。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;是否促进历史前进与是否受到群众理解是没有比例关系的。但对历史人物的认识是考量群众、社会进步的一个标杆，就像社会散沙度可通过电视随机提问展现一样。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;事情都有各自的周期，一类事情有一类的周期。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“大部分群众认可的事情是好事情”，是对“群众”的偶像化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;大势已成，则只能等事物沿着它的轨迹而行，是不以人的意志为转移的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;列强是指进口原材料，出口制成品；当外国的产品比本国的物美价廉时，对外国产品征收高额关税 来扶持本国这项技术的发展；对外要求自由竞争资本主义。如果各不发达国家不想当列弱了，必须实行贸易保护，自力更生，占领工业高地，建立自己的工业体系，这必然会遭到列强的干预和国内食利的腐朽集团的阻挠和镇压。其成败与否的关键在于三点：一是正确的领导党，二是群众的觉悟程度，三是知识分子与群众的结合程度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;贸易的最本质需求是互通有无。其正能量是促进生产力提高时，能更好地实现互通有无；其负能量是为生钱而生钱时，不以互通有无为目的，造成有撑着的、有饿着的、有营养过剩的、有虚胖的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;作者说过要“站在月球上来审视人类史”， 这是一本时间跨度长达400万年的历史巨著，所以他给我们的更多的应该是整个地球无尽岁月的点点缩影，让我们按照地理上的分布对不同地区不同时间不同文明有所了解。他更多承担的应该是兴趣入门。“历史的今天”，随时把历史上的重大变故跟当今世界的现状联系在一起，提醒着我们认清所生活的现实世界与历史的内在联系，从而使我们的思想能跨越时空的限制，在历史与现实的两个时空里驰骋，甚至由此产生自己出对历史事件的联想与对比，产生出自己的思想的火花与创作的冲动。由此让阅读历史，成为一种乐趣，成了一个对历史和现实两个世界的疑问同时不断探询和解答的过程。当我们对某个点感兴趣了，自己去查史料扩展自己的视野。匆匆读完一遍，随手记下感想。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://peihao.space/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://peihao.space/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>乌鸦你是谁</title>
    <link href="http://peihao.space/2017/01/25/17-01-25/"/>
    <id>http://peihao.space/2017/01/25/17-01-25/</id>
    <published>2017-01-25T14:46:11.000Z</published>
    <updated>2017-01-25T15:13:48.935Z</updated>
    
    <content type="html">&lt;p&gt;&lt;img src=&quot;http://7xowaa.com1.z0.glb.clouddn.com/wuya.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;center&gt;&lt;br&gt;&lt;br&gt;不知不觉就走到了这个尴尬的数字&lt;br&gt;&lt;br&gt;期盼着08，期盼着12，却原来已经在冷夜中蹒跚了数千个日日夜夜&lt;br&gt;&lt;br&gt;好嘛，也是把最初的幼稚与青涩用完的时候了吧&lt;br&gt;&lt;br&gt;年纪大了总会变得多愁善感&lt;br&gt;&lt;br&gt;本以为是云淡风轻，看着游客飘来荡去，顺着指尖，还是把最近的心思揉在一起丢到这里&lt;br&gt;&lt;br&gt;每个小站都是我不同的生活态度&lt;br&gt;&lt;br&gt;封存一些不想被熟知的你们探索的琐论杂情&lt;br&gt;&lt;br&gt;这里没有飞来飞去的广告&lt;br&gt;&lt;br&gt;没有生活所迫的编辑夺人眼球的标题&lt;br&gt;&lt;br&gt;我们用心记录所有随心所欲，不会因为假定的时时刻刻deadline画出没有意义的花瓶&lt;br&gt;&lt;br&gt;有的尽是自由的灵魂，祛除的都是非要不可&lt;br&gt;&lt;br&gt;&lt;br&gt;小站匆匆的陌生游客是否就是我所要的自由呢，我不在乎你怎么漂流到这里，也不会深究你是谁&lt;br&gt;&lt;br&gt;我是浅水中的一条河鱼，我就把这当成是我的自由罢&lt;br&gt;&lt;br&gt;对我而言，你就是深夜中扑腾的乌鸦，我们相遇，交谈或相离，不会记下彼此的记号&lt;br&gt;&lt;br&gt;&lt;br&gt;当期盼变成抗拒，当行进变成蹒跚&lt;br&gt;&lt;br&gt;别样的云清风淡，别样的人生海海&lt;br&gt;&lt;br&gt;&lt;br&gt;写在5位数之前，写在solar与lunar之间，写在惊醒与微醺之后&lt;br&gt;&lt;br&gt;不想懂没结果的是为什么&lt;br&gt;&lt;br&gt;不去问黑夜的乌鸦你是谁&lt;br&gt;&lt;br&gt;&lt;/center&gt;

&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;http://music.163.com/outchain/player?type=2&amp;id=185665&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xowaa.com1.z0.glb.clouddn.com/wuya.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;center&gt;&lt;br&gt;&lt;br&gt;不知不觉就走到了这个尴尬的数字&lt;br&gt;&lt;br&gt;期盼着08，期盼着12，却原来已经在冷夜中蹒跚了数千个日日夜夜&lt;br&gt;&lt;br&gt;好嘛，也是把最初的幼稚与青涩用完的时候了吧&lt;br&gt;&lt;br&gt;年纪大了总会变得多愁善感&lt;br&gt;&lt;br&gt;本以为是云淡风轻，看着游客飘来荡去，顺着指尖，还是把最近的心思揉在一起丢到这里&lt;br&gt;&lt;br&gt;每个小站都是我不同的生活态度&lt;br&gt;&lt;br&gt;封存一些不想被熟知的你们探索的琐论杂情&lt;br&gt;&lt;br&gt;这里没有飞来飞去的广告&lt;br&gt;&lt;br&gt;没有生活所迫的编辑夺人眼球的标题&lt;br&gt;&lt;br&gt;我们用心记录所有随心所欲，不会因为假定的时时刻刻deadline画出没有意义的花瓶&lt;br&gt;&lt;br&gt;有的尽是自由的灵魂，祛除的都是非要不可&lt;br&gt;&lt;br&gt;&lt;br&gt;小站匆匆的陌生游客是否就是我所要的自由呢，我不在乎你怎么漂流到这里，也不会深究你是谁&lt;br&gt;&lt;br&gt;我是浅水中的一条河鱼，我就把这当成是我的自由罢&lt;br&gt;&lt;br&gt;对我而言，你就是深夜中扑腾的乌鸦，我们相遇，交谈或相离，不会记下彼此的记号&lt;br&gt;&lt;br&gt;&lt;br&gt;当期盼变成抗拒，当行进变成蹒跚&lt;br&gt;&lt;br&gt;别样的云清风淡，别样的人生海海&lt;br&gt;&lt;br&gt;&lt;br&gt;写在5位数之前，写在solar与lunar之间，写在惊醒与微醺之后&lt;br&gt;&lt;br&gt;不想懂没结果的是为什么&lt;br&gt;&lt;br&gt;不去问黑夜的乌鸦你是谁&lt;br&gt;&lt;br&gt;&lt;/center&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://peihao.space/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://peihao.space/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>subprocess模块的使用</title>
    <link href="http://peihao.space/2016/12/18/subprocess-pipe/"/>
    <id>http://peihao.space/2016/12/18/subprocess-pipe/</id>
    <published>2016-12-18T14:09:48.000Z</published>
    <updated>2016-12-18T15:18:09.558Z</updated>
    
    <content type="html">&lt;p&gt;最近在考虑如何在一个独立的程序上加一个GUI壳子，原来的程序是通过console接受输入输出的，查阅资料发现了subprocess模块，这里记录一下。&lt;/p&gt;
&lt;h1 id=&quot;引入&quot;&gt;引入&lt;/h1&gt;&lt;p&gt;从名字就可以听出来，subprocess是python管理子进程的模块。运行python的时候，正常情况下我们都是在创建并运行一个进程。通过subprocess可以新建一个子进程执行程序，并通过subprocess提供的api与新创建的子进程联系。&lt;/p&gt;
&lt;p&gt;subprocess中最基础也最重要的就是基于Popen()函数，Popen()方法根据参数新建一个进程并执行，后面的一些列参数是对这个新创建的进程的管理。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;会话消息也就是I/O流，正常情况下有三种stdin、stdout以及stderr。前两个都很好理解，第三个就是标准的错误输出信息。&lt;/p&gt;
&lt;p&gt;Popen()方法通过对这三个参数的配置指定子进程I/O方式。一般支持的主要有两种方式：stdxx，这个也是默认的，就还是按原来的走；或者设置为Pipe，管道模式，这种模式下，子进程有一个缓冲区存储这输入输出信息，缓存等待着我们通过API进行数据的操作。这里实际上就是劫持了标准的I/O流。&lt;/p&gt;
&lt;h1 id=&quot;一些常用方法&quot;&gt;一些常用方法&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;terminate()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;停止(stop)子进程。在windows平台下，该方法将调用Windows API TerminateProcess（）来结束子进程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kill()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;杀死子进程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;communicate(input=None)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与子进程进行交互。向stdin发送数据，或从stdout和stderr中读取数据。可选参数input指定发送到子进程的参数。Communicate()返回一个元组：(stdoutdata, stderrdata)。注意：如果希望通过进程的stdin向其发送数据，在创建Popen对象的时候，参数stdin必须被设置为PIPE。同样，如果希望从stdout和stderr获取数据，必须将stdout和stderr设置为PIPE，这个方法与之前将标准I/O流设置成管道PIPE不同，communicate方法是在进程运行结束返回后将输出信息以及错误信息返回，在程序运行期间，无法通信。&lt;/p&gt;
&lt;h1 id=&quot;具体程序相关&quot;&gt;具体程序相关&lt;/h1&gt;&lt;p&gt;&lt;code&gt;p1=subprocess.Popen(cmd下执行程序的指令,stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将输入输出都设置为管道模式，错误信息重定向到输出，这样我们使用API获取输出信息时，能够直接拿到输出信息与错误信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;获取输出信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;s=p1.stdout.readline()&lt;/code&gt;&lt;br&gt;&lt;code&gt;s=s.decode(&amp;#39;gbk&amp;#39;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;从缓冲区中读取一行输出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;写入信息并实时的推送到原程序中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;p1.stdin.write(s.encode())&lt;/code&gt;&lt;br&gt;&lt;code&gt;p1.stdin.flush()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;因为劫持的标准I/O流，里面都是byte信息，所以为了能够使用，我们需要将byte转换成str，至于编码解码的编码集，需要根据服务器返回信息的不同测试设置。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;实时的获取反馈信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;独立于GUI线程，也不是在主线程中，新建一个专门查询缓冲区的线程，如果缓冲区有未接受的信息，通过Qt的信号-槽模式，将需要传递的信息发给GUI处理函数。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;最近在考虑如何在一个独立的程序上加一个GUI壳子，原来的程序是通过console接受输入输出的，查阅资料发现了subprocess模块，这里记录一下。&lt;/p&gt;
&lt;h1 id=&quot;引入&quot;&gt;引入&lt;/h1&gt;&lt;p&gt;从名字就可以听出来，subprocess是python管理子进程的模块。运行python的时候，正常情况下我们都是在创建并运行一个进程。通过subprocess可以新建一个子进程执行程序，并通过subprocess提供的api与新创建的子进程联系。&lt;/p&gt;
&lt;p&gt;subprocess中最基础也最重要的就是基于Popen()函数，Popen()方法根据参数新建一个进程并执行，后面的一些列参数是对这个新创建的进程的管理。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://peihao.space/categories/python/"/>
    
    
      <category term="python" scheme="http://peihao.space/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python3.x识别验证码</title>
    <link href="http://peihao.space/2016/12/11/valid/"/>
    <id>http://peihao.space/2016/12/11/valid/</id>
    <published>2016-12-11T11:55:21.000Z</published>
    <updated>2016-12-11T12:13:17.555Z</updated>
    
    <content type="html">&lt;p&gt;嗯，没心情写介绍了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;python2.x使用pytesser&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;python3.x使用pytesser3&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pytesser3 &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; image_to_string&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img=Image.open(&lt;span class=&quot;string&quot;&gt;r&#39;image_path&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(image_to_string(img))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;对于偏移过的元素支持不好&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;嗯，没心情写介绍了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;python2.x使用pytesser&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;python3.x使用pytesser3&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;ta
    
    </summary>
    
      <category term="python" scheme="http://peihao.space/categories/python/"/>
    
    
      <category term="python" scheme="http://peihao.space/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>系统结构读书笔记</title>
    <link href="http://peihao.space/2016/12/05/architecturere-readnote/"/>
    <id>http://peihao.space/2016/12/05/architecturere-readnote/</id>
    <published>2016-12-05T09:08:08.000Z</published>
    <updated>2016-12-06T15:00:57.047Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;《Computer Architecture A Quantitative Approach》部分读书笔记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本书的第二版，相比前面的版本，经过了一些修整。第一版的基础内容，第一章（Fundamentals of Computer Design）和第二章（Performance and Cost）在这个版本中，是在开篇第一章中（Fundamentals of Computer Design）介绍的，编者认为两者放在一起比独立的分开讲解效果更好一些。&lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;&lt;p&gt;计算机出现的几十年间，从20世纪70年代开始，微处理器的性能增长率逐年增长，从最开始的每年25%左右，20世纪70年代末期的35%。&lt;/p&gt;
&lt;p&gt;这样的增长速率以及大规模微处理器生产的成本优势，导致微处理器在计算机事业中的因素占比越来越大。同时，计算机市场的两个明显改变使得在一种新型的体系结构下实现了商业成功：&lt;/p&gt;
&lt;p&gt;首先，虚拟技术消除了原本的编程语言的装配过程，减少了母必爱代码的兼容性要求；另外标准化的创作、独立的操作系统（例如Unix）以及更低的成本和风险带来了一种新型的体系结构。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;这些改变使得继往开来，开发了一种新的体系结构集合，我们称它RISC体系结构。&lt;/p&gt;
&lt;p&gt;从RISC出现，80年代微处理器的性能增长率达到了50%以上，而且还在不断的提升。&lt;/p&gt;
&lt;p&gt;这种亮眼的增长率影响是双重的，首先明显的增强了能够提供给用户的发挥空间，用户可以享受在更低的时间内得到相同的结果；其次，这种恐怖的增长率导致了在计算机设计领域微处理为基础的主导地位：工作站和PC开始成为计算机行业的主要产品，小型机甚至超级计算机也逐渐采用利用多个微处理器代替原本的处理器。&lt;/p&gt;
&lt;h1 id=&quot;计算机设计的任务&quot;&gt;计算机设计的任务&lt;/h1&gt;&lt;p&gt;计算机设计者的任务客观来见是比较复杂的，一般有下面必须遵守：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;确定属性对于新机器的重要程度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在成本约数条件下最大限度的提高性能、设计机器  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然寥寥几个字，但是涉及到了很多方面：指令集的设计、功能组织、逻辑设计以及最后的实现部分。而实现部分又可能包括集成电路的设计、封装技术、电源甚至是温度管控等。&lt;/p&gt;
&lt;p&gt;设计者们设计一台计算机需要同时满足功能需求以及价格和性能上的目标。通常，他们也是功能需求的决策者，这可能是主要的任务。功能需要也许是来自市场方面的特别的特性，也许是计算机应用反向的驱动设计者考虑如何设计计算机。&lt;/p&gt;
&lt;p&gt;到之后的优化设计，需要的知识同样很多，编译器、操作系统、逻辑设计和包装等等，这些不仅需要计算机设计人员消耗大量的时间与精力，同时要求有很丰富的经验与知识广度。&lt;/p&gt;
&lt;p&gt;优化设计的指标有很多，最常见的指标设计成本和性能，在某些领域中，可靠性和容错性往往时相比成本和性能有更重要的地位，这里我们专注于常规领域，着重考虑机器的成本和性能。&lt;/p&gt;
&lt;h1 id=&quot;计算机与技术的使用趋势&quot;&gt;计算机与技术的使用趋势&lt;/h1&gt;&lt;p&gt;评判一个指令集的成功，一个重要的指标就是指令集必须能够在硬件技术、软件技术和应用特性的改变下表现不俗，所以设计师必须要特别了解计算机以及计算机技术的使用趋势。&lt;/p&gt;
&lt;h2 id=&quot;计算机使用趋势&quot;&gt;计算机使用趋势&lt;/h2&gt;&lt;p&gt;计算机的设计受两个方面影响：使用方式以及底层的实现技术特性。&lt;/p&gt;
&lt;p&gt;使用和实现技术的改变以不同的方式影响计算机设计，包括激励指令集的改变、流水方式还有类似缓存机制技术等等。&lt;/p&gt;
&lt;p&gt;软件技术的趋势以及程序是如何使用计算机对指令集结构有着长期的影响。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;程序所需的内存每年以1.5倍以上的速率增长，这种快速增长由程序的需求以及DRAM技术驱动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;高级语言逐渐替换汇编语言。这种趋势导致编译器承担更重要的作用，编译器的作者需要跟跟架构师们紧密的合作，构建一个更有竞争力的机器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;实现技术的趋势&quot;&gt;实现技术的趋势&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;集成电路逻辑技术：晶体管集成度每年都会提高约50%（晶体管的摩尔定律），在三年内翻了四倍，尺寸的增加可预测性不高，一般不会超过25%，然而布线技术并没有得到改善，这导致了时钟周期方面的速度提升缓慢。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;半导体DRAM集成度每年增长不超过60%，周期改善的相当缓慢，十年里减少了约三分之一，而芯片的带宽随着延迟的减少以及DRAM接口的改善得到提升。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;磁盘技术：1990年以前，磁盘的单位密度每年增长只有四分之一，90年以后，磁盘技术有了长足的发展，90年以后每年都有约一半的增加。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上这些快速变化的技术影响着微处理器的设计，随着速度和技术的增强，有长达5年以上的使用寿命。即使在单个产品周期（两年设计两年生产）范围内，关键的技术也足以影响设计者的决定；实际上，设计者经常会为下一个可能出现的技术设计。&lt;/p&gt;
&lt;h1 id=&quot;成本和成本趋势&quot;&gt;成本和成本趋势&lt;/h1&gt;&lt;h2 id=&quot;数目，商品化的影响&quot;&gt;数目，商品化的影响&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;数量是决定成本的关键因素，增加数量会在几个方面上影响成本。首先，他们减少了降低学习曲线（学习曲线学习曲线表示了经验与效率之间的关系，指的是越是经常地执行一项任务，每次所需的时间就越少。）所需的时间，减少时间的比率主要由使用的数目决定；其次，大批量的使用会降低成本，因为他增加了采购和制造的效率。根据经验，数目增加一倍，成本会降低一成。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;商品是由多个供应商大量销售并且基本相同的产品。不同的供应商对产品的高度竞争，降低了成本和售价之间的差距，同时也降低了计算机的成本。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;一个系统的成本分析&quot;&gt;一个系统的成本分析&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://7xowaa.com1.z0.glb.clouddn.com/architecture-cost.png&quot; alt=&quot;成本分布&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图是1990s末期彩色台式机的大致成本细目，虽然DRAM等设备的成本会随着时间的推移降低，但是部分设备的成本将会几乎没有变化。此外可以预期，未来的假期将会有更大的存储设备（内存或硬盘），这意味着价格下降比技术改进更慢。&lt;/p&gt;
&lt;p&gt;处理器系统仅占成本的6%，虽然在中高端设计中，比例会增加，但是在主要的系统中是类似的。&lt;/p&gt;
&lt;h1 id=&quot;性能的测量&quot;&gt;性能的测量&lt;/h1&gt;&lt;p&gt;当我们说一台电脑比另一台电脑快时，是什么意思？ 用户可以说，当程序以较少的时间运行时计算机更快；而计算机中心管理员可以说，在一小时内完成更多作业时计算机更快。计算机用户对减少事件的执行时间感兴趣。 大型数据处理中心的经理可能对增加吞吐量感兴趣。&lt;/p&gt;
&lt;p&gt;用户每天运行相同程序将是评估计算机性能的理想选择。为了评估新系统，用户需比较工作负载执行时间（用户在机器上运行的程序和操作系统命令的混合）。 然而，这种方法是机器枯燥的，大多数人依靠其他方法来评估，希望这些方法能够预测机器的使用性能。在这种情况下使用四个级别的程序，下面按照预测精度的降序列出。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;真实程序。虽然不知道这些程序花费了多少时间，但知道用户可能会运行他们解决实际问题。例如C的编译器，文本处理软件或者CAD工具。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;内核。尝试从真实的程序中提取几个小的、关键的片段评估他们的性能。Livemore Loops和Linpack是最出名的例子。与真实的程序不同，没有用户会运行内核程序，因为他们只用于评估性能。这种方法好在隔离机器各个特性的性能，解释实际程序性能差距。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Toy benchmarks。这种方法通常在数十行代码之间，产生用户在运行当前程序前已经知道的结果。像Sieve of Erastosthenes，Puzzle和Quicksort这样的程序很受欢迎，因为它们体积小，易于输入，几乎可以在任何计算机上运行。一般在程序语言开始分配阶段运行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Synthetic benchmarks。在原理上与内核很相似，这种方法试图匹配一个大的程序集里面的操作以及操作数平均值。Whetstone和Dhrystone是流行的产品。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;计算机设计的定量原理&quot;&gt;计算机设计的定量原理&lt;/h1&gt;&lt;h2 id=&quot;加快经常事例&quot;&gt;加快经常事例&lt;/h2&gt;&lt;p&gt;也许计算机设计的最重要和最普遍的原则是加快经常使用的cas额，这个原则也适用于确定如何使用资源，因为如果case发生频繁，对一些发生更快的影响想爱你然更大。 改进频繁事件，而不是罕见事件，也有助于性能提升。 此外，频繁的情况通常更简单，并且可以比不常见的情况更快地完成。&lt;/p&gt;
&lt;h2 id=&quot;Amdahl定律&quot;&gt;Amdahl定律&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Amdahl定律：系统中对某一部件采用更快执行方式所能获得的系统性能改进程度，取决于这种执行方式被使用的频率，或所占总执行时间的比例。阿姆达尔定律实际上定义了采取增强（加速）某部分功能处理的措施后可获得的性能改进或执行时间的加速比。简单来说是通过更快的处理器来获得加速是由慢的系统组件所限制。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;CPU性能方程&quot;&gt;CPU性能方程&lt;/h2&gt;&lt;p&gt;大多数计算机使用恒定速率运行的时钟构建。 这些离散时间事件称为时钟周期。计算机设计者通过其长度（例如，2ns）或其速率（例如，500MHz）来指代时钟周期的时间。一个程序的CPU时间可以表示为：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cpu time=时钟周期数 * 时钟周期时间&lt;/code&gt;&lt;/p&gt;
&lt;h1 id=&quot;内存分层概念&quot;&gt;内存分层概念&lt;/h1&gt;&lt;p&gt;首先，让我们看看硬件设计的一个经验：较小的硬件运行更快。这种简单的经验特别适用于由相同技术构建的存储器，原因有两个。首先，在高速机器中，信号传播是延迟的主要原因; 较大的存储器有更多的信号延迟，并且需要更多的电平来解码地址。第二，大多数技术中，我们可以获得比 较大的存储器 更快的 更小的存储器。这主要是因为设计者可以在更小的设计中使用每个存储器单元更多的功率。&lt;/p&gt;
&lt;p&gt;速表明有利于访问这样的数据将提高性能。 因此，我们应该尽量保持最近访问的项目在最快的内存中。 因为较小的存储器将更快，所以我们希望使用较小的存储器来尝试保持最近访问的项目靠近CPU，并且随着我们从CPU远离CPU而逐渐增大（和较慢）的存储器。 此外，对于更靠近CPU的那些存储器，我们还可以采用更昂贵和更高功率的存储器技术，因为它们小得多，并且由于存储器的小尺寸而降低了成本和功率影响。 这种类型的组织称为存储器层次结构。其中有两个重要的层次是缓存和虚拟内存。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xowaa.com1.z0.glb.clouddn.com/architecture-memory.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;（上图是上个世纪的容量与速度，不过到现在依然有参考意义）&lt;/p&gt;
&lt;p&gt;缓存是位于靠近CPU的小型快速存储器，其保存最近访问的代码或数据。 当CPU在高速缓存中找到所请求的数据项时，其被称为高速缓存命中。 当CPU在高速缓存中找不到它需要的数据项时，发生高速缓存未命中。 从主存储器检索包含所请求字的固定大小的称为块的数据块，并将其放入高速缓存中。 时间局部性告诉我们，我们可能在不久的将来需要这个词，因此将其放在可以被快速访问的缓存中是有用的。 由于空间局部性，块中的其他数据将很快需要的概率很高。&lt;/p&gt;
&lt;p&gt;高速缓存未命中所需的时间取决于存储器的延迟及其带宽，其确定检索整个块的时间。 由硬件处理的高速缓存未命中通常导致CPU暂停或停止，直到数据可用。&lt;/p&gt;
&lt;p&gt;同样，并非程序引用的所有对象都需要驻留在主存储器中。 如果计算机具有虚拟内存，则某些对象可能驻留在磁盘上。地址空间通常被分成固定大小的块，称为页面。&lt;/p&gt;
&lt;p&gt;在任何时候，每个页面驻留在主内存或磁盘上。 当CPU引用在高速缓存或主存储器中不存在的页面内的项目时，发生apage fault，并且整个页面从磁盘移动到主存储器。由于页面故障需要长时间，因此它们由软件处理，CPU不会停止。发生磁盘访问时，CPU通常切换到其他任务。 高速缓存和主存储器与主存储器和磁盘具有相同的关系。&lt;/p&gt;
&lt;h1 id=&quot;第二章部分介绍&quot;&gt;第二章部分介绍&lt;/h1&gt;&lt;p&gt;在本章中，我们专注于指令集架构。程序员或编译器写程序可见的机器部分。&lt;/p&gt;
&lt;p&gt;本章介绍了指令集架构师可用的各种各样的设计替代方案。特别的，本章重点讨论四个主题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;提出一个指令集替代品的分类，并给出一些定性评估各种方法的优缺点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;提出和分析一些指令集测量，这些测量在很大程度上独立于特定的指令集。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解决语言和编译器的问题，以及它们对指令集架构的影响。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;显示了以上想法如何反映在DLX指令集中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在本章中，研究了各种各样的结构测量方法。这些测量取决于测量的程序和用于进行测量的编译器。这些结果不是绝对的，因为如果使用不同的编译器或不同的程序集进行测量，可能会看到不同的数据。&lt;/p&gt;
&lt;p&gt;作者认为，这些章节中所示的测量值合理地表示了一类典型应用。许多测量使用一小组基准来呈现，使得可以合理地显示数据，并且可以看到之间的差异。&lt;/p&gt;
&lt;p&gt;一个新机器的架构师想要分析一个大的程序集合，来做出他的决策，所显示的所有测量都是动态的。也就是说，测量事件的频率是通过在执行测量程序期间事件发生的次数来加权计算得到。&lt;/p&gt;
&lt;h1 id=&quot;指令集结构的分类&quot;&gt;指令集结构的分类&lt;/h1&gt;&lt;p&gt;CPU内部存储器类型是指令结构分类考虑的关键指标，因此在本节中，将重点介绍这部分的替代方案。主要的选择有堆栈，累加器或一组寄存器。&lt;/p&gt;
&lt;p&gt;一种可以将存储器作为指令的一部分访问，称为寄存器-存储器架构，一种仅能够使用load/stroe指令访问主存的称之为寄存器-寄存器架构。第三类，目前流通的机器中还未出现，是将所有操作数保存在内存中，称为内存-内存架构。&lt;/p&gt;
&lt;p&gt;虽然大多数早期机器使用的是堆栈或累加器式架构，但在1980年后设计的机器几乎无一例外都采用load-store寄存器架构。&lt;/p&gt;
&lt;p&gt;出现通用目的寄存器机器的主要原因有两个。首先，CPU内部的寄存器的比存储器更快。第二，寄存器对于编译器来讲更容易使用，并且可以比其他形式的内部存储更有效地使用。&lt;/p&gt;
&lt;p&gt;寄存器可以用来保存变量。当变量被分配给寄存器时，存储器的使用就会减少，程序运行速度提高（由于寄存器比存储器快），并且代码密度提高（因为寄存器可以用比存储器位置更少的位命名）。&lt;/p&gt;
&lt;p&gt;两种主要的指令集特性划分了通用目的寄存器架构。这两个特性涉及典型算术与逻辑指令（ALU指令）的操作数的性质。第一个涉及ALU指令是否具有两个或三个操作数。在三操作数格式中，指令包含目的地址和两个源操作数。在双操作数格式中，操作数中的一个既是操作的源也是操作之后的结果。通用目的寄存器架构中的第二个区别涉及多少个操作数可以是ALU指令中的存储器地址。典型ALU指令支持的存储器操作数的数目从0到3。&lt;/p&gt;
&lt;h1 id=&quot;内存地址&quot;&gt;内存地址&lt;/h1&gt;&lt;p&gt;有两种不同的模式用于排序字内的字节。大端模式与小端模式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;大端模式，是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放；这和我们的阅读习惯一致。&lt;/p&gt;
&lt;p&gt;小端模式，是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，这种存储模式将地址的高低和数据位权有效地结合起来，高地址部分权值高，低地址部分权值低。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在许多机器中，对大于一个字节的对象访问必须对齐。什么是对齐？&lt;code&gt;address of data % sizeof(data type)==0&lt;/code&gt;就是对齐，对齐跟数据在内存中的位置有关。如果一个变量的内存地址正好位于它长度的整数倍，他就被称做自然对齐。比如在32位cpu下，假设一个整型变量的地址为0x00000004，那它就是自然对齐的。。不对齐就会造成数据访问花费额外的时钟周期，和额外的指令(编译器或OS附加的)，并且数据经过更长的路径，比如pipeline才能到达CPU（从RAM）。这就是对齐问题。这里的重点是数据的起始地址与数据的大小。&lt;/p&gt;
&lt;h2 id=&quot;寻址模式&quot;&gt;寻址模式&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;寄存器寻址模式&lt;/li&gt;
&lt;li&gt;立即寻址&lt;/li&gt;
&lt;li&gt;间接寻址&lt;/li&gt;
&lt;li&gt;索引寻址&lt;/li&gt;
&lt;li&gt;直接寻址或绝对寻址&lt;/li&gt;
&lt;li&gt;内存间接寻址&lt;/li&gt;
&lt;li&gt;变址寻址&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;小结&quot;&gt;小结&lt;/h1&gt;&lt;p&gt;本书第二版发型到现在虽然已经过了不短的时间，但是依然有很多值得我们细细品赏的地方。第一部分，作者没有像其他类似主题的书籍一样，上来就讲体系结构，而是拉着我们，从设计的趋势、设计的目的、设计架构需要考虑的诸多方面娓娓道来，读的过程中很有代入感，好像我们真的就是designer，特别是成本管控方面，原来基本上就没有考虑这一点，这里作者甚至从计算机制造业的视角分析了很多。这看起来跟我们没什么关系，但是实际上是为我们开辟了一个很棒的思路，以前很多模模糊糊的东西现在变得容易理解了。之后第二章开始讲了一些设计上的技术层面，第二章开始也说了，主要是关于指令集的知识，包括指令集结构划分的依据以及原因，字节存储模式、字节对齐的优势以及寻址模式等等，后面的内容由于时间关系没有看到，之后会继续品读，更新在这里。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;《Computer Architecture A Quantitative Approach》部分读书笔记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本书的第二版，相比前面的版本，经过了一些修整。第一版的基础内容，第一章（Fundamentals of Computer Design）和第二章（Performance and Cost）在这个版本中，是在开篇第一章中（Fundamentals of Computer Design）介绍的，编者认为两者放在一起比独立的分开讲解效果更好一些。&lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;&lt;p&gt;计算机出现的几十年间，从20世纪70年代开始，微处理器的性能增长率逐年增长，从最开始的每年25%左右，20世纪70年代末期的35%。&lt;/p&gt;
&lt;p&gt;这样的增长速率以及大规模微处理器生产的成本优势，导致微处理器在计算机事业中的因素占比越来越大。同时，计算机市场的两个明显改变使得在一种新型的体系结构下实现了商业成功：&lt;/p&gt;
&lt;p&gt;首先，虚拟技术消除了原本的编程语言的装配过程，减少了母必爱代码的兼容性要求；另外标准化的创作、独立的操作系统（例如Unix）以及更低的成本和风险带来了一种新型的体系结构。&lt;br&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://peihao.space/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://peihao.space/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>tornado安全技巧</title>
    <link href="http://peihao.space/2016/11/24/tornado-secure/"/>
    <id>http://peihao.space/2016/11/24/tornado-secure/</id>
    <published>2016-11-24T04:28:30.000Z</published>
    <updated>2016-11-24T15:22:40.594Z</updated>
    
    <content type="html">&lt;h1 id=&quot;安全cookies&quot;&gt;安全cookies&lt;/h1&gt;&lt;p&gt;Tornado的安全cookie使用加密签名验证cookies的值是否被除了服务器之外的程序修改过，未被我们授权的（不知道安全密钥）程序无法在应用不知情下修改cookies&lt;/p&gt;
&lt;h2 id=&quot;使用安全cookie&quot;&gt;使用安全cookie&lt;/h2&gt;&lt;p&gt;Tornado的&lt;code&gt;set_secure_cookie()&lt;/code&gt;和&lt;code&gt;get_secure_cookie()&lt;/code&gt;方法设置、请求浏览器的cookies，防止浏览器的恶意修改，在此之前，我们需要在构造函数中指定cookie_secure参数。&lt;/p&gt;
&lt;p&gt;成功设定之后，应用在程序内部获取的cookie还是本身的值，在浏览器或抓包工具查看，发现cookie已经被加密，如果不知道密钥，无法获知cookie内容&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;如果通过恶意代码修改过cookie值，应用使用get_secure_cookie方法会获得None，此时我们可以在应用中判断，增加一些防范机制&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.httpserver&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.ioloop&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.web&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.options&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tornado.options &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; define, options&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;define(&lt;span class=&quot;string&quot;&gt;&quot;port&quot;&lt;/span&gt;, default=&lt;span class=&quot;number&quot;&gt;8001&lt;/span&gt;, help=&lt;span class=&quot;string&quot;&gt;&quot;run on the given port&quot;&lt;/span&gt;, type=int)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;MainHandler&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(tornado.web.RequestHandler)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        times = self.get_secure_cookie(&lt;span class=&quot;string&quot;&gt;&quot;count&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        count = int(times) + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; times &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        countString = &lt;span class=&quot;string&quot;&gt;&quot;1 time&quot;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; count == &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;%d times&quot;&lt;/span&gt; % count&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.set_secure_cookie(&lt;span class=&quot;string&quot;&gt;&quot;count&quot;&lt;/span&gt;, str(count))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.write(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;string&quot;&gt;&#39;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Cookie Counter&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;string&quot;&gt;&#39;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;You’ve viewed this page %s times.&amp;lt;/h1&amp;gt;&#39;&lt;/span&gt; % countString + &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;string&quot;&gt;&#39;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        )&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;string&quot;&gt;&quot;__main__&quot;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tornado.options.parse_command_line()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    settings = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;cookie_secret&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;bZJc2sWbQLKos6GkHn/VB9oXwQt8S0R0kRvJ5/xJ89E=&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    application = tortnado.web.Application([&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        (&lt;span class=&quot;string&quot;&gt;r&#39;/&#39;&lt;/span&gt;, MainHandler)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ], **settings)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    http_server = tornado.httpserver.HTTPServer(application)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    http_server.listen(options.port)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tornado.ioloop.IOLoop.instance().start()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Tornado将cookie值编码为Base-64字符串&lt;/p&gt;
&lt;h2 id=&quot;http-only&amp;amp;&amp;amp;ssl&quot;&gt;http-only&amp;amp;&amp;amp;ssl&lt;/h2&gt;&lt;p&gt;Tornado的cookie功能依附于Python内建的Cokie模块，现在有两种常规的方法增强cookie的安全性，减少被截获的可能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为cookie设置&lt;code&gt;secure&lt;/code&gt;属性只指示浏览器只通过SSL传递cookie&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;self.set_cookie(&amp;#39;foo&amp;#39;,&amp;#39;bar&amp;#39;,secure=True)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开启Http-only功能&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;self.set_cookie(&amp;#39;foo&amp;#39;,&amp;#39;bar&amp;#39;,httponly=True)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;开启httponly之后，浏览器js不能访问cookie&lt;/p&gt;
&lt;p&gt;以上两种模式可以同时工作&lt;/p&gt;
&lt;h1 id=&quot;CSRF&quot;&gt;CSRF&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;CSRF（Cross-site request forgery跨站请求伪造，也被称为“One Click Attack”或者Session Riding，通常缩写为CSRF或者XSRF，是一种对网站的恶意利用。CSRF通过伪装来自受信任用户的请求来利用受信任的网站。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;举例&quot;&gt;举例&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;攻击通过在授权用户访问的页面中包含链接或者脚本的方式工作。例如：一个网站用户Bob可能正在浏览聊天论坛，而同时另一个用户Alice也在此论坛中，并且后者刚刚发布了一个具有Bob银行链接的图片消息。设想一下，Alice编写了一个在Bob的银行站点上进行取款的form提交的链接，并将此链接作为图片src。如果Bob的银行在cookie中保存他的授权信息，并且此cookie没有过期，那么当Bob的浏览器尝试装载图片时将提交这个取款form和他的cookie，这样在没经Bob同意的情况下便授权了这次事务&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;假设Alice是Burt’s Books的一个普通顾客。当她在这个在线商店登录帐号后，网站使用一个浏览器cookie标识她。现在假设一个不择手段的作者，Melvin，想增加他图书的销量。在一个Alice经常访问的Web论坛中，他发表了一个带有HTML图像标签的条目，其源码初始化为在线商店购物的URL。比如：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;img src=&amp;quot;http://store.burts-books.com/purchase?title=Melvins+Web+Sploitz&amp;quot; /&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Alice的浏览器尝试获取这个图像资源，并且在请求中包含一个合法的cookies，并不知道取代小猫照片的是在线商店的购物URL,点击这个url，就会提交一个购买Melvin书籍的请求&lt;/p&gt;
&lt;h2 id=&quot;防范措施&quot;&gt;防范措施&lt;/h2&gt;&lt;p&gt;有很多预防措施可以防止这种类型的攻击。首先你在开发应用时需要深谋远虑。任何会产生副作用的HTTP请求，比如点击购买按钮、编辑账户设置、改变密码或删除文档，都应该使用HTTP POST方法。&lt;/p&gt;
&lt;p&gt;但是，这并不足够：一个恶意站点可能会通过其他手段，如HTML表单或XMLHTTPRequest API来向你的应用发送POST请求。保护POST请求需要额外的策略。&lt;/p&gt;
&lt;p&gt;为了防范伪造POST请求，我们会要求每个请求包括一个参数值作为令牌来匹配存储在cookie中的对应值。我们的应用将通过一个cookie头和一个隐藏的HTML表单元素向页面提供令牌。当一个合法页面的表单被提交时，它将包括表单值和已存储的cookie。如果两者匹配，我们的应用认定请求有效。&lt;/p&gt;
&lt;p&gt;由于第三方站点没有访问cookie数据的权限，他们将不能在请求中包含令牌cookie。这有效地防止了不可信网站发送未授权的请求。正如我们看到的，Tornado同样会让这个实现变得简单。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;在构造函数中包含xsrf_cookies参数开启XSRF保护：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;settings = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;cookie_secret&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;bZJc2sWbQLKos6GkHn/VB9oXwQt8S0R0kRvJ5/xJ89E=&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;xsrf_cookies&quot;&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;application = tornado.web.Application([&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    (&lt;span class=&quot;string&quot;&gt;r&#39;/&#39;&lt;/span&gt;, MainHandler),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    (&lt;span class=&quot;string&quot;&gt;r&#39;/purchase&#39;&lt;/span&gt;, PurchaseHandler),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;], **settings)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这个应用标识被设置时，Tornado将拒绝请求参数中不包含正确的_xsrf值的POST、PUT和DELETE请求。为了在提交时候自动加上_xsrf值信息，我们在模板中使用如下：&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;form&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;action&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;/purchase&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;method&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;POST&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#123;% raw xsrf_form_html() %&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;title&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;quantity&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;submit&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;value&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;Check Out&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;form&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;网页在浏览器中的源码形式一般如下：&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;hidden&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;_xsrf&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;value&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;856ada5b160a9e27902f46631ce8e5d3&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Ajax请求也会需要_xsrf参数，通常是通过脚本在客户端查询浏览器获取cookie值。&lt;/p&gt;
&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;getCookie&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;name&lt;/span&gt;) &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;var&lt;/span&gt; c = &lt;span class=&quot;built_in&quot;&gt;document&lt;/span&gt;.cookie.match(&lt;span class=&quot;string&quot;&gt;&quot;\\b&quot;&lt;/span&gt; + name + &lt;span class=&quot;string&quot;&gt;&quot;=([^;]*)\\b&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; c ? c[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] : &lt;span class=&quot;literal&quot;&gt;undefined&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jQuery.postJSON = &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;url, data, callback&lt;/span&gt;) &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    data._xsrf = getCookie(&lt;span class=&quot;string&quot;&gt;&quot;_xsrf&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    jQuery.ajax(&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        url: url,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        data: jQuery.param(data),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        dataType: &lt;span class=&quot;string&quot;&gt;&quot;json&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type: &lt;span class=&quot;string&quot;&gt;&quot;POST&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        success: callback&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;安全cookies&quot;&gt;安全cookies&lt;/h1&gt;&lt;p&gt;Tornado的安全cookie使用加密签名验证cookies的值是否被除了服务器之外的程序修改过，未被我们授权的（不知道安全密钥）程序无法在应用不知情下修改cookies&lt;/p&gt;
&lt;h2 id=&quot;使用安全cookie&quot;&gt;使用安全cookie&lt;/h2&gt;&lt;p&gt;Tornado的&lt;code&gt;set_secure_cookie()&lt;/code&gt;和&lt;code&gt;get_secure_cookie()&lt;/code&gt;方法设置、请求浏览器的cookies，防止浏览器的恶意修改，在此之前，我们需要在构造函数中指定cookie_secure参数。&lt;/p&gt;
&lt;p&gt;成功设定之后，应用在程序内部获取的cookie还是本身的值，在浏览器或抓包工具查看，发现cookie已经被加密，如果不知道密钥，无法获知cookie内容&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://peihao.space/categories/python/"/>
    
    
      <category term="python" scheme="http://peihao.space/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>tornado2</title>
    <link href="http://peihao.space/2016/11/16/tornado2/"/>
    <id>http://peihao.space/2016/11/16/tornado2/</id>
    <published>2016-11-16T12:13:29.000Z</published>
    <updated>2016-11-18T14:11:45.455Z</updated>
    
    <content type="html">&lt;h1 id=&quot;模板基础&quot;&gt;模板基础&lt;/h1&gt;&lt;p&gt;与之前的Django类似的是，tornado同样是使用了MVT模式，通过模板渲染网页，tornado服务器端提取数据填充到模板文件中返回给client&lt;/p&gt;
&lt;p&gt;前面我们为了测试tornado异步框架以及最简单的request-response流程，没有使用Template，仅仅是简单的直接self.write写入到输出流返回给client&lt;/p&gt;
&lt;h2 id=&quot;py流程&quot;&gt;py流程&lt;/h2&gt;&lt;p&gt;实际的开发流程中，都会用到template模板，类似下面：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os.path&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.ioloop&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.options&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.web&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tornado.httpserver&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tornado.options &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; options,define&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;define(&lt;span class=&quot;string&quot;&gt;&#39;port&#39;&lt;/span&gt;,default=&lt;span class=&quot;number&quot;&gt;8001&lt;/span&gt;,helo=&lt;span class=&quot;string&quot;&gt;&#39;plz run on the given port&#39;&lt;/span&gt;,type=int)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;IndexHandler&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(tornado.web.RequestHandler)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.render(&lt;span class=&quot;string&quot;&gt;&#39;index.html&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;PoemHandler&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(tornado.web.RequestHandler)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        no1=self.get_argument(&lt;span class=&quot;string&quot;&gt;&#39;no1&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        no2=self.get_argument(&lt;span class=&quot;string&quot;&gt;&#39;no2&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        no3=self.get_argument(&lt;span class=&quot;string&quot;&gt;&#39;no3&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        verb=self.get_argument(&lt;span class=&quot;string&quot;&gt;&#39;verb&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.render(&lt;span class=&quot;string&quot;&gt;&#39;poem.html&#39;&lt;/span&gt;,no1=no1,no2=no2,no3=no3,verb-verb)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; __main__==&lt;span class=&quot;string&quot;&gt;&#39;__main__&#39;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tornodo.options.parse_command_line()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    app=tornado.web.Application(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    handler=[(&lt;span class=&quot;string&quot;&gt;r&#39;/&#39;&lt;/span&gt;,IndexHandler),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        (&lt;span class=&quot;string&quot;&gt;r&#39;/poem&#39;&lt;/span&gt;,PoemHandler)]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        template_path=os.path.join(os.path.dirname(__file__),&lt;span class=&quot;string&quot;&gt;&#39;templates&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    )&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    http_server=tornado.httpserver.HTTPServer(app)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    http_server.listen(options.port)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tornado.ioloop.IOLoop.instance().start()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;使用template之后，使用self.render代替了原有的self.write,render，提交给指定的html模板文件，并在方法中设置会在htlm文件中用到的context，然后tornodo就会把数据添到html文件中，之后把这个转换后的html文件作为response的body部分返回。&lt;/p&gt;
&lt;p&gt;这里用到了模板文件，发现这里是直接在render方法中使用文件名，也就是相对路径，因为我们在将所有需要监听的url加入到循环事件之前已经将template_path设置好了：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tempalte_path=os.path.join(os.path.dirname(__file____),&amp;#39;templates&amp;#39;)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;模板文件&quot;&gt;模板文件&lt;/h2&gt;&lt;p&gt;前面的os.path.dirname(&lt;strong&gt;file&lt;/strong&gt;)是将当前文件的目录提取出来，如果我们在执行这个程序的时候使用的是相对文件路径，那么返回的当然也是相对路径，总之就是返回可以识别的路径，然后后面加上’templates’这个目录名称，这个目录是提前创建好的，里面有两个模板文件：index.html和poem.html&lt;/p&gt;
&lt;p&gt;index.html文件是很简单的一个包含form表格的html:&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;&amp;lt;!--简单的写一个大题流程--&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;body&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;form&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;action&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;/poem&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;method&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;post&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;no1&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;no2&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;no3&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;verb&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;attribute&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;value&quot;&gt;&quot;submit&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;form&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;body&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到，我们的表单action是/poem，也就是说同构submit按钮提交会转给server_adderss:port/poem这个路径处理，正好对用着我们在Application中设置的url，&lt;code&gt;(r&amp;#39;/poem&amp;#39;,PoemHandler)&lt;/code&gt;，这里，实际上index.html完全就是普通的html文件，跟正常的文件没有丝毫区别&lt;/p&gt;
&lt;p&gt;然后是使用模板流程的poem.html&lt;br&gt;&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;body&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    no1 --- &amp;#123;&amp;#123;no1&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    no2 --- &amp;#123;&amp;#123;no2&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    no3 --- &amp;#123;&amp;#123;no3&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    verb --- &amp;#123;&amp;#123;verb&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;body&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;这里就用到了模板文件中常用的数据占位符号&lt;code&gt;{}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;我们在往上面的PoemHandler类的post方法看看那：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    no1=self.get_argument(&lt;span class=&quot;string&quot;&gt;&#39;no1&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    no2=self.get_argument(&lt;span class=&quot;string&quot;&gt;&#39;no2&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.render(&lt;span class=&quot;string&quot;&gt;&#39;poem.html&#39;&lt;/span&gt;,no1=no1,no2=no2,no3=no3,verb=verb)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;由于是使用的post方法提交的转到的/poem页面，我们这里使用的是post方法处理数据（其他方法没有用）&lt;/p&gt;
&lt;p&gt;self.get_argument这个方法没什么好讲的，简单通俗，从post发来的信息中听过argument name提取数据，这里的参数名就是index.html表格中的input name=”…” 这个&lt;/p&gt;
&lt;p&gt;&lt;code&gt;self.render(&amp;#39;poem.html&amp;#39;,no1=no1....)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里传递的context一定要设置好，即使我们在这里故意使用了相同的参数name，如果不在render方法中设置，服务器会认为你传递的是空的context，就像IndexHandler中的get方法一样，不需要使用context。&lt;/p&gt;
&lt;p&gt;我们在模板文件中用两个大括号包裹的就是设置的context的左值，右值是我们之前处理的数据。&lt;/p&gt;
&lt;h1 id=&quot;模板扩展&quot;&gt;模板扩展&lt;/h1&gt;&lt;p&gt;tornado中的模板扩展，包括extens和block块使用与Django都很相似，因为有前面的知识，这里我们简单过一下&lt;/p&gt;
&lt;h2 id=&quot;extends&quot;&gt;extends&lt;/h2&gt;&lt;p&gt;原始的base文件base.html中使用的所有tag、layout、css、js内容都能够在使用扩展命令的其他模板文件中直接使用：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{ % extends base.html % }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这一点与Django完全一致&lt;/p&gt;
&lt;h2 id=&quot;block&quot;&gt;block&lt;/h2&gt;&lt;p&gt;只是继承原始的文件有什么用呢，一定要重写某些部分，够则与原始文件毫无二致，这需要2个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在原始需要扩展的base.html中的某些需要替换部分使用&lt;code&gt;{ % block block_name % }{ % end % }&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在扩展的文件中&lt;code&gt;{ % extends base.html % }&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在扩展的文件中重写&lt;code&gt;block_name&lt;/code&gt;部分&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;base.html&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;&amp;lt;!--base.html--&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hiahiahia&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; % block content % &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    I am the base.html&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; % end % &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;override.html&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;&amp;lt;!--override.html--&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; % extends &quot;base.html&quot; % &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; % block content % &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;I am the override.html&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; % end % &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;override.html文件继承base.html文件内容，在content block块中修改相应的内容&lt;/p&gt;
&lt;p&gt;Tips:因为在base.html中我们定义的是完整的html标签，我们不可能破坏了原有的整体布局，当然也可以在块中继续使用html标签、引用css、js等&lt;/p&gt;
&lt;p&gt;这样我们在分别请求对应的url后会发现显示的不同的内容&lt;/p&gt;
&lt;p&gt;与Django做对比，Django模板中也有block部分，使用也相当相似：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{ % block content % }{ % endblock % }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在Django中因为要与for循环的结束符号&lt;code&gt;{ % endfor % }&lt;/code&gt;作出区别，没有直接使用end，而是选择了endblock，其他的毫无区别&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;模板基础&quot;&gt;模板基础&lt;/h1&gt;&lt;p&gt;与之前的Django类似的是，tornado同样是使用了MVT模式，通过模板渲染网页，tornado服务器端提取数据填充到模板文件中返回给client&lt;/p&gt;
&lt;p&gt;前面我们为了测试tornado异步框架以及最简单的request-response流程，没有使用Template，仅仅是简单的直接self.write写入到输出流返回给client&lt;/p&gt;
&lt;h2 id=&quot;py流程&quot;&gt;py流程&lt;/h2&gt;&lt;p&gt;实际的开发流程中，都会用到template模板，类似下面：&lt;br&gt;
    
    </summary>
    
      <category term="tornado" scheme="http://peihao.space/categories/tornado/"/>
    
    
      <category term="tornado" scheme="http://peihao.space/tags/tornado/"/>
    
  </entry>
  
  <entry>
    <title>uwsgi</title>
    <link href="http://peihao.space/2016/11/16/uwsgi/"/>
    <id>http://peihao.space/2016/11/16/uwsgi/</id>
    <published>2016-11-16T03:27:38.000Z</published>
    <updated>2016-11-18T12:17:35.350Z</updated>
    
    <content type="html">&lt;h1 id=&quot;wsgi&quot;&gt;wsgi&lt;/h1&gt;&lt;p&gt;先来讲讲Web应用，抛开dns、网络连接这些先不谈，我们只看服务器与浏览器client之间，静态Web应用一般是这样的流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;浏览器发送一个http请求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;服务器收到request，生成或者找到请求的文件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;服务器将html文件作为response的body部分返回/无需渲染文件直接返回&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;浏览器收到响应后，从response取出数据并填充、渲染&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;类似Apache、Nginx这类static server最擅长干的事情，就是我们提前将浏览器要访问的html页面、static文件（img、css、js）等放到静态服务器的指定位置，服务器接受请求返回文件&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;动态服务器的话因为要根据请求的url解析并生成数据返回给浏览器，上面的一些步骤就需要我们自己实现。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不过，接受HTTP请求、解析HTTP请求、发送HTTP响应都是苦力活，如果我们自己来写这些底层代码，还没开始写动态HTML呢，就得花个把月去读HTTP规范。&lt;/p&gt;
&lt;p&gt;正确的做法是底层代码由专门的服务器软件实现，我们用Python专注于生成HTML文档。因为我们不希望接触到TCP连接、HTTP原始请求和响应格式，所以，需要一个统一的接口，让我们专心用Python编写Web业务。&lt;/p&gt;
&lt;p&gt;这个接口就是WSGI：Web Server Gateway Interface。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;WSGI接口定义非常简单，它只要求Web开发者实现一个函数，就可以响应HTTP请求。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#app.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;application&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(environ,start_response)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	start_response(&lt;span class=&quot;string&quot;&gt;&#39;200 OK&#39;&lt;/span&gt;,[(&lt;span class=&quot;string&quot;&gt;&#39;Content-Type&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;text/html&#39;&lt;/span&gt;)])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	body=&lt;span class=&quot;string&quot;&gt;&#39;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;hiahia&amp;lt;/title&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello %s&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/head&amp;gt;&#39;&lt;/span&gt; % (environ[&lt;span class=&quot;string&quot;&gt;&#39;PATH_INFO&#39;&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:] &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;world&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; [body.encode(&lt;span class=&quot;string&quot;&gt;&#39;utf-8&#39;&lt;/span&gt;)]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这个函数是被wsgi服务器调用，跑在wsgi服务器上的，接受两个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;environ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;包含所有HTTP请求信息的dict对象，上面使用的environ[‘PATH_INFO’]就是我们请求的服务器地址，例如服务器跑在本机端口8001上:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl http://localhost:8001/hiahiahia/hiahiahia&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;那么此时就会返回 Hello hiahiahia/hiahiahia，如果是直接请求/，会返回默认设置的’world’&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;start_response&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;发送HTTP响应的函数&lt;/p&gt;
&lt;p&gt;application()函数中，调用：&lt;code&gt;start_response(&amp;#39;200 OK&amp;#39;, [(&amp;#39;Content-Type&amp;#39;, &amp;#39;text/html&amp;#39;)])&lt;/code&gt;&lt;br&gt;就发送了HTTP响应的Header，注意Header只能发送一次，也就是只能调用一次start_response()函数。start_response()函数接收两个参数，一个是HTTP响应码，一个是一组list表示的HTTP Header，每个Header用一个包含两个str的tuple表示。多个元祖之间以逗号分开，填充到list中&lt;/p&gt;
&lt;p&gt;然后，函数的返回值b’&lt;/p&gt;&lt;h1&gt;Hello, web!&lt;/h1&gt;‘将作为HTTP响应的Body发送给浏览器。&lt;p&gt;&lt;/p&gt;
&lt;p&gt;有了WSGI，我们关心的就是如何从environ这个dict对象拿到HTTP请求信息，然后构造HTML，通过start_response()发送Header，最后返回Body。&lt;/p&gt;
&lt;p&gt;整个application()函数本身没有涉及到任何解析HTTP的部分，也就是说，底层代码不需要我们自己编写，我们只负责在更高层次上考虑如何响应请求就可以了。&lt;/p&gt;
&lt;p&gt;application()函数必须由WSGI服务器来调用，我们这里简单测试就用python自带的wsgiref&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#server.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; wsgiref.simple_server &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; make_server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; app &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; application&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;httpd=make_server(&lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;8001&lt;/span&gt;,application)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;httpd.serve_forever()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;无论多么复杂的Web应用程序，入口都是一个WSGI处理函数。HTTP请求的所有输入信息都可以通过environ获得，HTTP响应的输出都可以通过start_response()加上函数返回值作为Body。&lt;/p&gt;
&lt;h1 id=&quot;uWSGI&quot;&gt;uWSGI&lt;/h1&gt;&lt;p&gt;uwsgi是uWSGI服务器自创的一种协议，是一种线路协议而非类似wsgi的通信协议。&lt;/p&gt;
&lt;p&gt;uWSGI是实现了WSGI、uwsgi和http协议的服务器&lt;/p&gt;
&lt;h2 id=&quot;django+uWSGI+nginx&quot;&gt;django+uWSGI+nginx&lt;/h2&gt;&lt;p&gt;这里吧django、uWSGI和nginx的关系梳理一下，网络上有大把的文章都是在描述这三者怎么搭载、怎么执行的，却没有好好介绍之间的联系：&lt;/p&gt;
&lt;p&gt;Django实际上只是一个流行的开源框架，因为其高效、便捷全面的开发流程收到欢迎；&lt;/p&gt;
&lt;p&gt;nginx为Django Project提供反向代理服务，是对外服务的接口，浏览器通过url请求服务器的时候，请求首先会经过nginx，nginx将请求分析，如果是静态文件则直接返回可用的静态文件，动态网页就转发给uwsgi；&lt;/p&gt;
&lt;p&gt;uwsgi收到请求后处理成wsgi能够接受的格式，wsgi根据配置文件使用app的函数，然后把处理后的数据在打包成uwsgi接受的格式、转发给nginx-&amp;gt;传回给浏览器&lt;/p&gt;
&lt;p&gt;从上面的流程中可以看到，实际上nginx做的处理静态文件请求这件事，uwsgi也是能做的，那么为什么还要用nginx呢？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;对于静态文件的处理，nginx要比uwsgi好得多&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;程序不希望被浏览器访问到，而是通过nginx,nginx只开放某个接口，uwsgi本身则是内网接口，这样运维人员在nginx上加上安全性的限制，可以达到保护程序的作用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nginx可以做反向映射，一个uwsgi服务器可能不能保证程序的运行，放到多个服务器上，这样通过nginx配置文件中配置后，不同的app转发到不同的uwsgi上。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;关于反向映射，可以看这张图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/proxy.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于普通代理来讲，代理机与client(browser)属于同一个域，服务器将他们视为整体，代理的是client；&lt;/p&gt;
&lt;p&gt;反向代理与真正的服务器在一个域，属于同一局域网，client将他们视为整体，代理的是后面多台服务器，主要有cache、安全性以及负载平衡方面的考虑&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;wsgi&quot;&gt;wsgi&lt;/h1&gt;&lt;p&gt;先来讲讲Web应用，抛开dns、网络连接这些先不谈，我们只看服务器与浏览器client之间，静态Web应用一般是这样的流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;浏览器发送一个http请求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;服务器收到request，生成或者找到请求的文件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;服务器将html文件作为response的body部分返回/无需渲染文件直接返回&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;浏览器收到响应后，从response取出数据并填充、渲染&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;类似Apache、Nginx这类static server最擅长干的事情，就是我们提前将浏览器要访问的html页面、static文件（img、css、js）等放到静态服务器的指定位置，服务器接受请求返回文件&lt;br&gt;
    
    </summary>
    
      <category term="网络" scheme="http://peihao.space/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络" scheme="http://peihao.space/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Linux 网络IO模式(转)</title>
    <link href="http://peihao.space/2016/11/15/io_multiplexing/"/>
    <id>http://peihao.space/2016/11/15/io_multiplexing/</id>
    <published>2016-11-15T15:28:30.000Z</published>
    <updated>2016-11-16T08:59:04.679Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000003063859&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Linux IO模式-人云思云&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;IO模式&quot;&gt;IO模式&lt;/h1&gt;&lt;p&gt;对于一次IO访问，数据会先被拷贝到操作系统的内核缓冲中，然后应用会从缓冲区中获取数据，这中间有两个过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;等待数据准备（waiting for data to be read）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将数据从内核拷贝到进程中(copying the data from the kernel to the process)&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;一般有5种网络模式：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;阻塞&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;非阻塞&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I/O多路复用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;信号驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;异步I/O&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;阻塞_I/O（blocking_IO）&quot;&gt;阻塞 I/O（blocking IO）&lt;/h2&gt;&lt;p&gt;在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/async/block_io.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。&lt;/p&gt;
&lt;p&gt;所以，blocking IO的特点就是在IO执行的两个阶段都被block了。&lt;/p&gt;
&lt;h2 id=&quot;非阻塞_I/O（nonblocking_IO）&quot;&gt;非阻塞 I/O（nonblocking IO）&lt;/h2&gt;&lt;p&gt;linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/async/nonblock_io.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。&lt;/p&gt;
&lt;p&gt;在这种模式下，之前提到的两种过程：等待数据拷贝到内核以及从内核拷贝到应用中，在第一个过程，进程并未被阻塞，依然可以继续进行下去，但是只能去不断的轮询数据是否到达了；第二个过程依然还是阻塞形式。&lt;/p&gt;
&lt;h2 id=&quot;异步IO&quot;&gt;异步IO&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/async/asynchronous_io.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。&lt;/p&gt;
&lt;h2 id=&quot;I/O_多路复用（_IO_multiplexing）&quot;&gt;I/O 多路复用（ IO multiplexing）&lt;/h2&gt;&lt;p&gt;IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/async/io_multiplexing.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。&lt;/p&gt;
&lt;p&gt;所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。&lt;br&gt;这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。&lt;/p&gt;
&lt;p&gt;所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）&lt;/p&gt;
&lt;p&gt;在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。&lt;/p&gt;
&lt;h1 id=&quot;asynchronous/synchronous_IO&quot;&gt;asynchronous/synchronous IO&lt;/h1&gt;&lt;p&gt;是同步IO还是异步IO在POSIX中是这样定义的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;&lt;/p&gt;
&lt;p&gt;An asynchronous I/O operation does not cause the requesting process to be blocked;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。&lt;/p&gt;
&lt;p&gt;有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。&lt;/p&gt;
&lt;p&gt;而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/async/io_comparison.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;多路复用的3种方式&quot;&gt;多路复用的3种方式&lt;/h1&gt;&lt;p&gt;select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间，异步I/O根本不会在数据复制的过程中接触到这些数据。&lt;/p&gt;
&lt;h2 id=&quot;select&quot;&gt;select&lt;/h2&gt;&lt;p&gt;&lt;code&gt;int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符，然后再一同使用recvfrom函数从描述符指引的地方拿数据。&lt;/p&gt;
&lt;p&gt;select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。&lt;/p&gt;
&lt;h2 id=&quot;poll&quot;&gt;poll&lt;/h2&gt;&lt;p&gt;&lt;code&gt;int poll (struct pollfd *fds, unsigned int nfds, int timeout);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;struct&lt;/span&gt; pollfd &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; fd; &lt;span class=&quot;comment&quot;&gt;/* file descriptor */&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;short&lt;/span&gt; events; &lt;span class=&quot;comment&quot;&gt;/* requested events to watch */&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;short&lt;/span&gt; revents; &lt;span class=&quot;comment&quot;&gt;/* returned events witnessed */&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。&lt;/p&gt;
&lt;p&gt;从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。&lt;/p&gt;
&lt;h2 id=&quot;epoll&quot;&gt;epoll&lt;/h2&gt;&lt;p&gt;epoll操作过程需要三个接口，分别如下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;int epoll_create(int size)&lt;/code&gt;&lt;br&gt;创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。&lt;br&gt;当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)&lt;/code&gt;&lt;br&gt;函数是对指定描述符fd执行op操作。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;epfd：是epoll_create()的返回值。&lt;/li&gt;
&lt;li&gt;op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。&lt;/li&gt;
&lt;li&gt;fd：是需要监听的fd（文件描述符）&lt;/li&gt;
&lt;li&gt;epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;等待epfd上的io事件，最多返回maxevents个事件。&lt;br&gt;参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。&lt;/p&gt;
&lt;h2 id=&quot;工作模式&quot;&gt;工作模式&lt;/h2&gt;&lt;p&gt;epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：
　　&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。
　　&lt;/li&gt;
&lt;li&gt;ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;LT模式&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ET模式&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)&lt;/p&gt;
&lt;p&gt;ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;epoll优点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。&lt;/p&gt;
&lt;h1 id=&quot;事件循环&quot;&gt;事件循环&lt;/h1&gt;&lt;p&gt;nginx和tornado的最高效实现实际上都是基于epoll或类似epoll（mac下是kqueue，windows下只支持select）的事件循环机制&lt;/p&gt;
&lt;p&gt;异步事件驱动模型中，把会导致阻塞的操作转化为一个异步操作，主线程负责发起这个异步操作，并处理这个异步操作的结果。由于所有阻塞的操作都转化为异步操作，理论上主线程的大部分时间都是在处理实际的计算任务，少了多线程的调度时间，所以这种模型的性能通常会比较好。&lt;/p&gt;
&lt;p&gt;事件驱动编程的架构是预先设计一个事件循环，这个事件循环程序不断地检查目前要处理的信息，根据要处理的信息运行一个触发函数。其中这个外部信息可能来自一个目录夹中的文件，可能来自键盘或鼠标的动作，或者是一个时间事件。这个触发函数，可以是系统默认的也可以是用户注册的回调函数。&lt;/p&gt;
&lt;p&gt;事件驱动程序设计着重于弹性以及异步化上面。&lt;/p&gt;
&lt;p&gt;基于事件驱动的编程是单线程思维，其特点是异步+回调。&lt;strong&gt;异步的实现可以使使用多线程，也可以将任务交给其他进程处理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;协程也是单线程，但是它能让原来要使用异步+回调方式写的非人类代码,可以用看似同步的方式写出来。它是实现推拉互动的所谓非抢占式协作的关键。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000003063859&quot;&gt;Linux IO模式-人云思云&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;IO模式&quot;&gt;IO模式&lt;/h1&gt;&lt;p&gt;对于一次IO访问，数据会先被拷贝到操作系统的内核缓冲中，然后应用会从缓冲区中获取数据，这中间有两个过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;等待数据准备（waiting for data to be read）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将数据从内核拷贝到进程中(copying the data from the kernel to the process)&lt;/p&gt;
    
    </summary>
    
      <category term="网络" scheme="http://peihao.space/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="linux" scheme="http://peihao.space/tags/linux/"/>
    
      <category term="网络" scheme="http://peihao.space/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
